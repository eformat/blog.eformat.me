<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    
    <title>eformat.me - stable diffusion</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="eformat.me : stable diffusion">
    <meta property="og:title" content="eformat.me - stable diffusion" />
    <meta property="og:type" content="website" />
    <meta property="og:image" content="https://blog.eformat.me/img/eformat.me.jpg" />
    <meta property="og:url" content="https://blog.eformat.me/tags/stable diffusion.html" />
    <meta property="og:description" content="eformat.me : stable diffusion" />
    <meta property="og:locale" content="en_GB" />
    <meta property="og:site_name" content="eformat.me" />

    <!-- Le styles -->
    <link href="../css/lightbox.css" rel="stylesheet">
    <link href="../css/yeti/bootstrap.min.css" rel="stylesheet">
    <link href="../css/base.css" rel="stylesheet">
    <link href="../css/asciidoctor.css" rel="stylesheet">
    <!-- link href="/css/bootstrap-theme.min.css" rel="stylesheet" -->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.2.0/css/all.min.css">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
      <script src="https://oss.maxcdn.com/libs/respond.js/1.3.0/respond.min.js"></script>
    <![endif]-->

    <!-- Fav and touch icons -->
    <!--<link rel="apple-touch-icon-precomposed" sizes="144x144" href="../assets/ico/apple-touch-icon-144-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="114x114" href="../assets/ico/apple-touch-icon-114-precomposed.png">
    <link rel="apple-touch-icon-precomposed" sizes="72x72" href="../assets/ico/apple-touch-icon-72-precomposed.png">
    <link rel="apple-touch-icon-precomposed" href="../assets/ico/apple-touch-icon-57-precomposed.png">-->
    <link rel="shortcut icon" href="../favicon.ico">
  </head>
  <body>
    <div id="wrap">

	
	      <!-- Fixed navbar -->
      <div class="navbar navbar-default navbar-fixed-top" role="navigation">
        <div class="container-fluid">
          <div class="navbar-header">
            <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#navbar-menu">
              <span class="sr-only">Toggle navigation</span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <a class="navbar-brand" href="../index.html">eformat.me</a>
          </div>
          <div class="collapse navbar-collapse" id="navbar-menu">
            <ul class="nav navbar-nav">
              <li><a href="https://github.com/eformat" target="github:eformat">Github</a></li>
              <li><a href="../archive.html">Archives</a></li>
              <li><a href="../feeds/posts/default.xml">Flux RSS</a></li>
            </ul>
          </div><!--/.nav-collapse -->
        </div>
      </div>
      <div class="container">


	<div class="page-header">
            <div class="row">
                <div class="col-xs-4 col-md-2"><img src="../img/eformat.me.jpg"></div>
                <div class="col-xs-12 col-md-8"><h1>Tag: stable diffusion</h1></div>
            </div>
	</div>

    <div class="row">

    <div class="col-sm-8">
        
            
                <a href="../2022/12/nvidia-gpu-sharing.html"><h1>Stable Diffusion on OpenShift with GPU Sharing</h1></a>
                <p>13 December 2022</p>

                <p>Tags :
                <a href="openshift.html">openshift</a>, <a href="gpu.html">gpu</a>, <a href="aiml.html">aiml</a>, <a href="stable diffusion.html">stable diffusion</a>
                </p>

                <!--a href="https://twitter.com/share" class="twitter-share-button" data-url="http://www.eformat.me/2022/12/nvidia-gpu-sharing.html" data-text="Stable Diffusion on OpenShift with GPU Sharing" data-via="eformat" data-lang="en">Tweeter</a-->
                <!--script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script-->
                <div class="g-plusone" data-size="medium" data-href="http://www.eformat.me/2022/12/nvidia-gpu-sharing.html"></div>

                <p><div class="sect1">
<h2 id="_stable_diffusion_on_openshift_with_gpu_sharing">Stable Diffusion on OpenShift with GPU Sharing</h2>
<div class="sectionbody">
<div class="paragraph">
<p>So the intuitive follow on from the last blog post <a href="https://blog.eformat.me/2022/11/stable-diffusion.html">Stable Diffusion for Fedora Core</a> is of course to see if we can get the app running on OpenShift in a lab environment!</p>
</div>
<div class="paragraph">
<p>There are a couple of challenges. In my case, i actually wanted to demo the app in a lab that contains some older <a href="https://www.nvidia.com/en-au/data-center/tesla-t4/">Nvidia-Tesla-T4 GPU&#8217;s</a>, a bare metal SNO instance along with a bunch of other GPU enabled apps. This raises some interesting questions, in particular how do we configure and deploy applications so they can share the GPU&#8217;s in this environment?</p>
</div>
<div class="paragraph">
<p>One of the best article i found <a href="https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes">describing GPU Sharing</a> and the various mechanisms involved, highlights the different options available.</p>
</div>
<div id="lightbox"></div>
<div class="imageblock id="gpu-concurrency-mechanisms">
  <img src="/2022/12/gpu-concurrency-mechanisms.png" class="zoom">
</div>
<div class="paragraph">
<p>We are interested primarily in the system software and hardware part of this picture (CUDA and MPS-CUDA are more at the application level). Although, Stable Diffusion does require working CUDA for python torch as well.</p>
</div>
<div class="paragraph">
<p><code>MIG</code> (which stands for multi instance GPU) is the newest technology and only supported on a small number of cards (not the T4') like vGPU (A100 and A30). There are some great <a href="https://www.openshift.com/blog/multi-instance-gpu-support-with-the-gpu-operator-v1.7.0">OpenShift blogs</a> describing MIG usage. vGPU is a technology that is <strong>only</strong> available if OpenShift is running in a VM/hypervisor. vGPUs are created/configured at the hypervisor level independently of OpenShift.</p>
</div>
<div class="paragraph">
<p>So, that leaves us with <strong>Time-slicing</strong>. The <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/time-slicing-gpus-in-openshift.html#configuring-gpus-with-time-slicing">best place to read about it</a> is on the Nvidia site. Unlike MIG, there is no memory or fault-isolation between replicas, but for some workloads this is better than not being able to share the GPU at all. <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-sharing.html">There is a lot of documentation</a> to read, so i&#8217;m going to summarize the steps to get OpenShift Bare Metal SNO working using time-slicing.</p>
</div>
<div class="sect2">
<h3 id="_installing_the_node_feature_discovery_nfd_operator">Installing the Node Feature Discovery (NFD) Operator</h3>
<div class="paragraph">
<p>The first step after installing OpenShift SNO bare-metal, was to configure the NFD operator as cluster-admin. The default configuration for the operator is fine. All going well, your GPU&#8217;s should now be visible to OpenShift, and you can check by doing:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ oc debug node/&lt;node name&gt;
$ chroot /host
$ lspci | grep -i nvidia
17:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
65:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can see our two physical GPU&#8217;s OK. Another check is the node labels and description:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ oc describe node | egrep 'Roles|pci' | grep -v master
   feature.node.kubernetes.io/pci-10de.present=true</code></pre>
</div>
</div>
<div class="paragraph">
<p>If you see the <strong>pci-10de</strong> device, that is the code for Nvidia GPU&#8217;s, all good so far.</p>
</div>
</div>
<div class="sect2">
<h3 id="_installing_the_nvidia_gpu_operator">Installing the NVIDIA GPU Operator</h3>
<div class="paragraph">
<p>Next step is to <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html">install the Nvidia GPU Operator</a>. By default you should <strong>not</strong> need to install any license as <a href="https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions">OpenShift 4.9.9+ is entitlement free</a>. There are several pods that install with this operator. If you install the default <code>Cluster Policy</code> the nvidia driver is downloaded and compiled for your OpenShift and inserted as dynamic <strong>kmods</strong>. This may take a little bit of time to complete.</p>
</div>
<div id="nvidia-driver" class="paragraph">
<p><span class="image"><img src="/2022/12/nvidia-driver-pod.png" alt="Nvidia Dameon Set" width="640" height="480"></span></p>
</div>
<div class="paragraph">
<p>In our case, we only have one node (SNO) so the dameon set compiles and installs the driver on our node. If you follow the documentation above you should be able to verify the drivers are loaded.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ oc debug node/&lt;node name&gt;
$ chroot /host
$ lsmod | grep nvidia
nvidia_modeset       1142784  0
nvidia_uvm           1310720  2
nvidia              40796160  363 nvidia_uvm,nvidia_modeset
drm                   589824  4 drm_kms_helper,nvidia,mgag200</code></pre>
</div>
</div>
<div class="paragraph">
<p>Its worth noting that if you were using vGPU, you would <strong>also</strong> get the <em>nvidia_vgpu_vfio</em> module, but because we are bare metal, the driver dameon set recognizes passthrough mode and does not compile it.</p>
</div>
<div class="paragraph">
<p>The second part of the puzzle is you need to now configure the GPU for time-slicing. To do this we need create a ConfigMap that specifies how many slices we want, for example <em>8</em> in our case.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="yaml">kind: ConfigMap
apiVersion: v1
metadata:
  name: time-slicing-config
  namespace: nvidia-gpu-operator
data:
  tesla-t4: |-
    version: v1
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 8</code></pre>
</div>
</div>
<div class="paragraph">
<p>Next, we add this ConfigMap name into the nvidia.com ClusterPolicy.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="yaml">                  devicePlugin:
                    config:
                      default: "tesla-t4"
                      name: "time-slicing-config"
                    enabled: true</code></pre>
</div>
</div>
<div class="paragraph">
<p>By enabling the <em>devicePlugin</em> you should see the device plugin DaemonSet spin up.</p>
</div>
<div id="nvidia-deive-plugin" class="paragraph">
<p><span class="image"><img src="/2022/12/nvidia-device-plugin.png" alt="Nvidia Device Plugin Dameon Set" width="640" height="480"></span></p>
</div>
<div class="paragraph">
<p>We are nearly there ! If we now look at the OpenShift node description, we should see how many GPU&#8217;s OpenShift now thinks it has.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ oc describe node| sed '/Capacity/,/System/!d;/System/d'

Capacity:
  ...
  nvidia.com/gpu:                 16
Allocatable:
  ...
  nvidia.com/gpu:                 16</code></pre>
</div>
</div>
<div class="paragraph">
<p>So great ! that is <strong>8x2=16</strong> time-sliced GPU&#8217;s available.</p>
</div>
</div>
<div class="sect2">
<h3 id="_deploy_stable_diffusion">Deploy Stable Diffusion</h3>
<div class="paragraph">
<p>I have created a simple <a href="https://github.com/eformat/stable-diffusion/tree/main/openshift">Kustomize folder</a> in the git repo and split out the two part needed to get the app running.</p>
</div>
<div class="paragraph">
<p>First create a data download job (this is 6 GB of downloads), which creates a PVC using he default Storage Class to download the required Stable Diffusion model data.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">oc apply -f create-data/app.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then run the deployment.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">oc apply -f create-app/app.yaml</code></pre>
</div>
</div>
<div class="paragraph">
<p>Here&#8217;s an example of a run on the lab, showing the <code>nvidia-smi pmon</code> on the shell for the running python process and an output text to image.</p>
</div>
<div class="imageblock id="stable-diffusion-gpu-time-slice">
  <img src="/2022/12/stable-diffusion-gpu-time-slice.png" class="zoom">
</div>
<div class="paragraph">
<p>In our Deployment we only requested one GPU, so we get one time-sliced gpu.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="yaml">        resources:
          limits:
            nvidia.com/gpu: 1</code></pre>
</div>
</div>
<div class="paragraph">
<p>You can scale this up, or use the nvidia sample image to test out time-slicing and sharing e.g. Create a Deployment using this image.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="yaml">        replicas: 16
        image: nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04
        resources:
          limits:
            nvidia.com/gpu: "1"</code></pre>
</div>
</div>
<div class="paragraph">
<p>And hey presto ! we now see 15/16 app replicas spinning up and running on our 2 physical GPU&#8217;s. You can see them easily using <code>nvidia-smi pmon</code>. We don&#8217;t quite get to 16 as Stable Diffusion is still running on the GPU as well!</p>
</div>
<div class="imageblock id="stable-diffusion-gpu-time-slice">
  <img src="/2022/12/gpu-sharing-16.png" class="zoom">
</div>
</div>
</div>
</div></p>
                <p><a href="2022/12/nvidia-gpu-sharing.html#disqus_thread">Commentaires</a></p>
            

        
            
                <a href="../2022/11/stable-diffusion.html"><h1>Stable Diffusion for Fedora Core</h1></a>
                <p>23 November 2022</p>

                <p>Tags :
                <a href="fedora.html">fedora</a>, <a href="gpu.html">gpu</a>, <a href="aiml.html">aiml</a>, <a href="stable diffusion.html">stable diffusion</a>
                </p>

                <!--a href="https://twitter.com/share" class="twitter-share-button" data-url="http://www.eformat.me/2022/11/stable-diffusion.html" data-text="Stable Diffusion for Fedora Core" data-via="eformat" data-lang="en">Tweeter</a-->
                <!--script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script-->
                <div class="g-plusone" data-size="medium" data-href="http://www.eformat.me/2022/11/stable-diffusion.html"></div>

                <p><div class="sect1">
<h2 id="_stable_diffusion_for_fedora_core">Stable Diffusion for Fedora Core</h2>
<div class="sectionbody">
<div class="paragraph">
<p>If you have not heard about it, <code>Stable Diffusion</code> <a href="https://stability.ai/blog/stable-diffusion-public-release">is a text to image ML model generator</a>. I wanted to demo a GPU with podman and OCI like a pro and I don&#8217;t want to use to the awesome but boring <code>docker.io/nvidia/samples:vectoradd-cuda11.2.1</code></p>
</div>
<div class="paragraph">
<p>Watching numbers add .. erm, yeah. This is 2022 baby!</p>
</div>
<div class="paragraph">
<p>So let&#8217;s see if we can build this <code>Stable Diffusion</code> thing on fedora. The setup is <em>painfull</em> .. I warn you now. But <em>its worth the effort</em> - trust me&#8230;&#8203;</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_getting_setup">Getting Setup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>I have a 4 (nearly 5 ?) year old work laptop - a <code>dell-xps-15</code>. It has been a real workhorse 🐴 ! But, if you have been running fedora for as long as i have, you will know that running NVIDIA graphics has been .. well, torturous to say the least over the years. Things have gotten <em>way better</em>.</p>
</div>
<div id="linus-nvidia" class="paragraph">
<p><span class="image"><img src="/2022/11/linus-nvidia.jpg" alt="Linus " width="640" height="480"></span></p>
</div>
<div class="paragraph">
<p>So strap yourself in ! At least these days, NVIDIA play very nicely in the Open Source community, so hopefully you will not need to <em>"do a Linus"</em> as we probably all have over the years.</p>
</div>
<div class="paragraph">
<p>Here&#8217;s my hardware devices:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ lspci | egrep -i 'vga|nvidia'
00:02.0 VGA compatible controller: Intel Corporation HD Graphics 630 (rev 04)
01:00.0 3D controller: NVIDIA Corporation GP107M [GeForce GTX 1050 Mobile] (rev a1)</code></pre>
</div>
</div>
<div class="paragraph">
<p>Its worth noting that i run my display with the perfectly acceptable <strong>intel i915 gpu</strong> (on the mother board). For one thing, it runs a lot cooler than the NVIDIA card, so less noisy overall as the fans don&#8217;t scream. You can blacklist the nvidia drivers then <code>dracut -f</code> the boot image:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ cat /etc/modprobe.d/blacklist_video.conf
blacklist nouveau
blacklist lbm-nouveau
blacklist nvidia-current
alias nvidia nvidia_drm nvidia_modeset nvidia_current_updates
alias nouveau off
alias lbm-nouveau off</code></pre>
</div>
</div>
<div class="paragraph">
<p>OR, you can set up the kernel to boot using <code>i915.modeset</code> and blacklist there as well. I also blacklist the default <code>nouveau</code> driver because, err - it runs like a dog! You never want to use it when you have other options, like two perfectly good graphics card drivers to choose from!</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ cat /etc/default/grub
GRUB_CMDLINE_LINUX="i915.modeset=1 quiet rhgb intel_iommu=on modprobe.blacklist=nouveau,nvidia,nvidia_drm,nvidia_modeset rd.driver.blacklist=nouveau"</code></pre>
</div>
</div>
<div class="paragraph">
<p>One other thing i find very handy, is to run graphics using the intel gpu, and then use the NVIDIA gpu for AIML, or to pass through via PCI to OpenShift SNO running in <code>libvirt</code>, so i set <code>intel_iommu=on</code> as well for good measure. This seems to confuse people, but if you want to pci passthrough the device to libvirt &#8230;&#8203; you <strong>cannot</strong> share it ! i.e. don&#8217;t run your main monitor using the NVIDIA card, and expect to share it with a VM using pci-passthrough.</p>
</div>
<div class="paragraph">
<p>Make sure to recreate your (in my case UEFI) bootloader <code>grub2-mkconfig -o /boot/efi/EFI/fedora/grub.cfg</code> if you change any of these.</p>
</div>
<div class="paragraph">
<p>Sadly, you are not done. What you need now is a running NVIDIA, CUDA drivers for your operating system. I&#8217;m running fc36. So many years, so much pain here, so many crappy blogs giving you bad advice. OK .. so this is my advice, if you are starting with a broken system, <code>dnf erase nvidia*</code> is your best bet. Start from a clean state.</p>
</div>
<div class="paragraph">
<p>For fc36, use the f35 repo. There is also a f36 repo <a href="https://forums.developer.nvidia.com/t/bug-report-on-nvidia-driver-515-65-01-for-fedora-36-kernel-5-18-19-rtx-2060-rev-1/227009/7">that is known not to work!</a>. Why is this? i don&#8217;t know, i have not debugged the C/C++ yet, but <code>dkms</code> will fail to compile the kernel driver if you try the fc36 repo (and the nvidia driver version is lower, so go figure?).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/fedora35/x86_64/cuda-fedora35.repo
dnf -y module install nvidia-driver:latest-dkms
dnf -y install cuda</code></pre>
</div>
</div>
<div class="paragraph">
<p>Now, you will also need (cuDNN) which is a GPU-accelerated library of primitives for deep neural networks. The easiest way i found to install this is to grab the local repo as rpm and install it. You <a href="https://developer.nvidia.com/rdp/cudnn-archive">need to download it here from nvidia</a>.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">dnf -y install cudnn-local-repo-rhel8-8.5.0.96-1.0-1
tree /var/cudnn-local-repo-rhel8-8.5.0.96/
dnf install -y /var/cudnn-local-repo-rhel8-8.5.0.96/libcudnn8-8.5.0.96-1.cuda11.7.x86_64.rpm
dnf erase cudnn-local-repo-rhel8-8.5.0.96-1.0-1</code></pre>
</div>
</div>
<div class="paragraph">
<p>Once you have <code>libcudnn</code> installed, you can uninstall the local repo. There may be a better way, but 🤷</p>
</div>
<div class="paragraph">
<p>I use a simple shell script to load my nvidia driver when i need it.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ cat ~/bin/nvidia.sh
#!/bin/bash
# we have these blacklisted on boot so we can load i915
sudo dkms status
sudo modprobe nvidia_drm modeset=1 nvidia_modeset nvidia
sudo ldconfig</code></pre>
</div>
</div>
<div class="paragraph">
<p>For <code>podman</code>, you will need to do the following</p>
</div>
<div class="ulist">
<ul>
<li>
<p>install nvidia drivers as above, make sure <code>nvidia-smi</code> works on the host (see testing in next section)</p>
</li>
<li>
<p>Install <code>nvidia-container-toolkit</code></p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">curl -s -L https://nvidia.github.io/libnvidia-container/rhel8.6/libnvidia-container.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo
dnf install -y nvidia-container-toolkit</code></pre>
</div>
</div>
<div class="ulist">
<ul>
<li>
<p>Modify <code>/etc/nvidia-container-runtime/config.toml</code> and change these values (needed because of <code>cgroupsv2</code> and the desire to run the pod rootless if possible)</p>
</li>
</ul>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">[nvidia-container-cli]
#no-cgroups = false
no-cgroups = true
#user = root:video
user = "root:root"
[nvidia-container-runtime]
#debug = "/var/log/nvidia-container-runtime.log"
debug = "~/./local/nvidia-container-runtime.log"</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should now be good to go.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_test_your_local_setup">Test Your Local Setup</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Let&#8217;s get some things working. Needless to say if any of these steps fail, you are going to have to debug and fix them 🛠️ !</p>
</div>
<div class="paragraph">
<p>This is always my first check, from your shell:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ nvidia-smi

Wed Nov 23 05:21:19 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   56C    P8    N/A /  N/A |      0MiB /  4096MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+</code></pre>
</div>
</div>
<div class="paragraph">
<p>If podman setup correctly this will also work in a pod (note this is rootless and done as my normal user):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">podman run --rm --security-opt=label=disable \
--hooks-dir=/usr/share/containers/oci/hooks.d/ \
docker.io/nvidia/cuda:11.2.2-base-ubi8 \
/usr/bin/nvidia-smi</code></pre>
</div>
</div>
<div class="paragraph">
<p>We can now check that a python container will find your GPU and CUDA setup correctly. Stable Diffusion uses the <a href="http://torch.ch/">torch</a> library, but if things don&#8217;t work tensorflow gives you a lot more details about any failure (libraries, cuda version mismatch etc). It is worth pointing out you <strong>must</strong> have the same CUDA libs in both places (your host and image), so make sure you <strong>do</strong>! (see the Dockerfile for Stable Diffusion later on).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">podman run --rm -it --privileged \
--security-opt=label=disable \
--hooks-dir=/usr/share/containers/oci/hooks.d/ \
docker.io/tensorflow/tensorflow:latest-gpu</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should be able to check that the <code>nvidia</code> device is available in the pod:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">root@3e8c8ba4e6fb:/# ls -lart /dev/nvidia0
crw-rw-rw-. 1 nobody nogroup 195, 0 Nov 23 01:26 /dev/nvidia0</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then check that tensorflow can see your GPU, this will give you detailed messages if it cannot find your drivers and libraries:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">root@3e8c8ba4e6fb:/# python3.8
Python 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0] on linux
Type "help", "copyright", "credits" or "license" for more information.
&gt;&gt;&gt; import tensorflow as tf
2022-11-23 06:37:46.901772: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
...
&gt;&gt;&gt; tf.test.gpu_device_name()
2022-11-23 06:37:52.706585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 3364 MB memory:  -&gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
'/device:GPU:0'</code></pre>
</div>
</div>
<div class="paragraph">
<p>The last line <code>'/device:GPU:0'</code> is good. Now, we can also check torch works (you can leave this check till later, once you have built or pulled the <code>Stable Diffusion</code> image)</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">$ pip3.8 install torch --user
$ python3.8 -c "import torch; print(torch.cuda.is_available())"
True</code></pre>
</div>
</div>
<div class="paragraph">
<p>If that returns False, then something is amiss.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_build_the_stable_diffusion_image">Build the Stable Diffusion Image</h2>
<div class="sectionbody">
<div class="paragraph">
<p>OK, the instructions from here should be straight forward and i have <a href="https://github.com/eformat/stable-diffusion/">put the instructions in a git repo here</a>. Strictly speaking you can just grab the image and run it if you have a similar setup to mine <code>podman pull quay.io/eformat/sd-auto:14-02</code>. Be warned its a 6GB image!</p>
</div>
<div class="paragraph">
<p>You will need some time ☕ and storage available! The AI model downloads use approx (12GB) of local disk 😲 and we use the <code>aria2</code> torrent client to grab all the bits needed.</p>
</div>
<div class="paragraph">
<p>Download the data.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">dnf -q install aria2
./download.sh</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then we can build the container using podman. There is a <code>Makefile</code> to make your life easier.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">make build</code></pre>
</div>
</div>
<div class="paragraph">
<p>Then we can run it using podman. Note: you have to mount the <code>download/data</code> folder so set <code>DATA_DIR=&lt;full path&gt;/download/data</code> appropriately. We also run the pod as privileged which should not ne strictly be necessary (/dev/nvidia0 is not found otherwise, this needs fixing up).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="bash">podman run --privileged -it -p 7860:7860 -e CLI_ARGS="--allow-code --medvram --xformers" \
-v $DATA_DIR:/data:Z \
--security-opt=label=disable \
--hooks-dir=/usr/share/containers/oci/hooks.d/ \
quay.io/eformat/sd-auto:14-02</code></pre>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_demo_it">Demo It!</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Browse to <code><a href="http://0.0.0.0:7860/" class="bare">http://0.0.0.0:7860/</a></code> and type in some text. In this example i was using:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="text">forest wanderer by dominic mayer, anthony jones, Loish, painterly style by Gerald parel, craig mullins, marc simonetti, mike mignola, flat colors illustration, bright and colorful, high contrast, Mythology, cinematic, detailed, atmospheric, epic , concept art, Matte painting, Lord of the rings, Game of Thrones, shafts of lighting, mist, , photorealistic, concept art, volumetric light, cinematic epic + rule of thirds</code></pre>
</div>
</div>
<div class="paragraph">
<p>You should get an awesome image generated!</p>
</div>
<div class="paragraph">
<p><span class="image"><img src="/2022/11/tmpcgvezq90.png" alt="Image " width="640" height="480"></span></p>
</div>
<div class="paragraph">
<p>You can also check the python process is running using your GPU OK by running:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="prettyprint highlight"><code data-lang="text">$ nvidia-smi pmon</code></pre>
</div>
</div>
<div class="paragraph">
<p>Which you can see with the application and shell side by side here.</p>
</div>
<div id="lightbox"></div>
<div class="imageblock id="ddd-school-timetable-classes">
  <img src="/2022/11/stable-diffusion.png" class="zoom">
</div>
<div class="paragraph">
<p>🎉🎉 Enjoy 🎉🎉</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="_attribution">Attribution</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Code was forked with 💕 from here. Check it out if you want to build other UI&#8217;s to demo with.</p>
</div>
<div class="paragraph">
<p><a href="https://github.com/AbdBarho/stable-diffusion-webui-docker" class="bare">https://github.com/AbdBarho/stable-diffusion-webui-docker</a></p>
</div>
</div>
</div></p>
                <p><a href="2022/11/stable-diffusion.html#disqus_thread">Commentaires</a></p>
            

        

    </div>

    <div class="col-sm-3 col-sm-offset-1 blog-sidebar">
        <div class="sidebar-module sidebar-module-inset">
            <h4>Mike Hepburn</h4>
            <p>This is really working out great.</p>
            <ul>
                <li><a href="https://twitter.com/eformat">@eformat</a></li>
            </ul>
        </div>


        <div class="sidebar-module">
            <h4>Tags</h4>
            <ol class="list-unstyled" style="margin-left: 0px">
                

                <li><a href="openshift.html">openshift</a> (7)</li>
                

                <li><a href="aiml.html">aiml</a> (2)</li>
                

                <li><a href="argocd.html">argocd</a> (2)</li>
                

                <li><a href="gitops.html">gitops</a> (2)</li>
                

                <li><a href="gpu.html">gpu</a> (2)</li>
                

                <li><a href="java.html">java</a> (2)</li>
                

                <li><a href="stable diffusion.html">stable diffusion</a> (2)</li>
                

                <li><a href="acm.html">acm</a> (1)</li>
                

                <li><a href="aws.html">aws</a> (1)</li>
                

                <li><a href="bgp.html">bgp</a> (1)</li>
                

                <li><a href="bird.html">bird</a> (1)</li>
                

                <li><a href="constraints.html">constraints</a> (1)</li>
                

                <li><a href="cost.html">cost</a> (1)</li>
                

                <li><a href="devops.html">devops</a> (1)</li>
                

                <li><a href="fediverse.html">fediverse</a> (1)</li>
                

                <li><a href="fedora.html">fedora</a> (1)</li>
                

                <li><a href="flink.html">flink</a> (1)</li>
                

                <li><a href="frr.html">frr</a> (1)</li>
                

                <li><a href="mastodon.html">mastodon</a> (1)</li>
                

                <li><a href="metallb.html">metallb</a> (1)</li>
                

                <li><a href="optaplanner.html">optaplanner</a> (1)</li>
                

                <li><a href="patterns.html">patterns</a> (1)</li>
                

                <li><a href="platform.html">platform</a> (1)</li>
                

                <li><a href="platform as product.html">platform as product</a> (1)</li>
                

                <li><a href="pulsar.html">pulsar</a> (1)</li>
                

                <li><a href="quarkus.html">quarkus</a> (1)</li>
                

                <li><a href="security.html">security</a> (1)</li>
                

                <li><a href="sno.html">sno</a> (1)</li>
                

                <li><a href="social.html">social</a> (1)</li>
                

                <li><a href="streaming.html">streaming</a> (1)</li>
                

                <li><a href="vault.html">vault</a> (1)</li>
                
            </ol>
        </div>
    </div>

    </div>

		</div>
		<div id="push"></div>
    </div>
    
    <div id="footer">
      <div class="container">
        <p class="text-muted credit">Blog posts are published under Creative Commons license by-nc-sa <em>Creative Commons by-nc-sa</em>. <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="http://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"/></a></p>
        <p class="text-muted credit">&copy; 2022 | Baked with <a href="http://jbake.org">JBake v2.7.0-rc.6</a><a href="https://github.com/eformat/blog.eformat.me/actions"> | Published Fri Feb 17 09:22:14 UTC 2023</a></p>
      </div>
    </div>
    
    <!-- Javascript
    ================================================== -->
    <!-- Placed at the end of the document so the pages load faster -->
    <script src="../js/lightbox.js"></script>

    <!--script type="text/javascript">
        window.___gcfg = {lang: 'en'};

        (function() {
            var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
            po.src = 'https://apis.google.com/js/platform.js';
            var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
        })();
    </script-->

    <!--script>
      (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
      (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
      m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
      })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

      ga('create', 'UA-', '');
      ga('send', 'pageview');

    </script-->
    
  </body>
</html>

