<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>eformat.me</title>
    <link>https://blog.eformat.me</link>
    <atom:link href="https://blog.eformat.me/feeds/posts/default.xml" rel="self" type="application/rss+xml" />
    <description>eformat.me</description>
    <language>en-gb</language>
      <pubDate>Thu, 10 Nov 2022 05:09:14 +0000</pubDate>
      <lastBuildDate>Thu, 10 Nov 2022 05:09:14 +0000</lastBuildDate>

      
      <item>
          <title>AI Constraints Programming with Quarkus and OptaPlanner</title>
          <link>https://blog.eformat.me/2022/11/optaplanner-quarkus.html</link>
          <pubDate>Thu, 10 Nov 2022 05:09:03 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/optaplanner-quarkus.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_ai_on_quarkus_i_love_it_when_an_optaplan_comes_together&quot;&gt;AI on Quarkus: I love it when an OptaPlan comes together&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have been meaning to look at &lt;a href=&quot;https://www.optaplanner.org/&quot;&gt;OptaPlanner&lt;/a&gt; for ages. All i can say is &lt;em&gt;&quot;Sorry Geoffrey De Smet, you are a goddamn genius and i should have played with OptaPlanner way sooner&quot;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, i watched this video to see how to get started.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;videoblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/3N3NoDs3Ylc?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So much fun ! üòÅ to code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;quarkus-school-timetabling-screenshot&quot;&gt;
  &lt;img src=&quot;/2022/11/quarkus-school-timetabling-screenshot.png&quot; class=&quot;zoom&quot;&gt;
  &lt;div class=&quot;title&quot;&gt;Figure - Quarkus School Timetable&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There were a couple of long learnt lessons i remembered whilst playing with the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;(1) Domain Driven Design&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To get at the heart of constraints programming you need a good object class hierarchy, one that is driven by your business domain. Thanks Eric Evans for the gift that keeps giving - DDD (and UML) is perfect to help you out here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;ddd-school-timetable-classes&quot;&gt;
  &lt;img src=&quot;/2022/11/ddd-school-timetable-classes.png&quot; class=&quot;zoom&quot;&gt;
  &lt;div class=&quot;title&quot;&gt;Figure - Class Hierarchy&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You need to have a clean and well thought out class heirarchy so that wiring in OptaPlanner will work for you. I can see several iterations and workshop sessions ensuing to get to a workable and correct understanding of the problem domain.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;(2) Constraints Programming&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I went looking for some code i helped write some 15 years ago ! A &lt;a href=&quot;https://en.wikipedia.org/wiki/Constraint_programming&quot;&gt;Constraint&lt;/a&gt; based programming model we had written in C++&lt;/p&gt;
&lt;/div&gt;
&lt;script src=&quot;https://gist.github.com/eformat/87efba0e2ec717ff077852c5924766ec.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We had a whole bunch of Production classes used for calculating different trades types and their values. You added these &lt;em&gt;productions&lt;/em&gt; into a solver class heirarcy and if you had the right &lt;a href=&quot;https://en.wikipedia.org/wiki/Degrees_of_freedom&quot;&gt;degrees of freedom&lt;/a&gt; your trade calculation would be successful. The beauty of it was the solver would spit out any parameter you had not specified, as long as it was possible to calculate it based on the production rules.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OptaPlanner viscerally reminded me of that code and experience, and started me thinking about how to use it for a similar use case.&lt;/p&gt;
&lt;/div&gt;
&lt;script src=&quot;https://gist.github.com/eformat/6551fe9434bb0a810321c83bd07adee2.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I am now a fan ü•∞&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One last lesson from the OptaPlanner crew was their use of a a new static doc-generation system, their docs are a thing of beauty i have to say, &lt;a href=&quot;https://jbake.org&quot;&gt;JBake&lt;/a&gt; which I am using to write this blog with. Thanks for all the fish üêü üê† Geoff.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OptaPlanner Quickstarts Code - &lt;a href=&quot;https://github.com/kiegroup/optaplanner-quickstarts&quot; class=&quot;bare&quot;&gt;https://github.com/kiegroup/optaplanner-quickstarts&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>DevOps with OpenShift Introduction</title>
          <link>https://blog.eformat.me/2022/11/devops-with-openshift-5yr.html</link>
          <pubDate>Thu, 10 Nov 2022 05:09:03 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/devops-with-openshift-5yr.html</guid>
          <description>
              &lt;div class=&quot;sect1 pagenumrestart&quot;&gt;
&lt;h2 id=&quot;Introduction-to-DevOps&quot;&gt;5 Years!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It&amp;#8217;s been five years since we wrote the inaugural DevOps with OpenShift book. I re-read the introduction recently, and thought &lt;em&gt;&quot;It hasn&amp;#8217;t aged that badly !&quot;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://cloud.redhat.com/hubfs/pdfs/DevOps_with_OpenShift.pdf&quot; class=&quot;bare&quot;&gt;https://cloud.redhat.com/hubfs/pdfs/DevOps_with_OpenShift.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;cover&quot; class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/2022/11/dowo_cover.png&quot; alt=&quot;DevOps with OpenShift&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;title&quot;&gt;Figure 1. DevOps with OpenShift&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_introduction_to_devops_with_openshift&quot;&gt;Introduction to DevOps with OpenShift&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This book provides a practical guide for using OpenShift as an enablement technology for DevOps. OpenShift&amp;#8217;s combination of container management platform with natively container-aware automation can bring those Developer and Operations constituencies together in ways not previously possible. This enables software work products to present themselves in a standardized form to your preferred continuous integration and delivery tool chains.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Container awareness makes it possible to leverage deployment strategies and quality of service characteristics honored by the container management platform and underlying orchestration engine. We can start thinking in terms of &lt;em&gt;containers-as-code&lt;/em&gt; rather than &lt;em&gt;infrastructure-as-code&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So to get started, let&amp;#8217;s review some key DevOps concepts as interpreted with a container-centric viewpoint.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_devops&quot;&gt;DevOps&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;DevOps is concerned with aligning the constituents in the software delivery process to a common goal of value delivery&amp;#x2014;and it&amp;#8217;s not just Developers and Operators, but InfoSec and Quality Assurance functions and more. Recognize that wealth is created when the work product is valued by actors external to the production system. Value delivery outcomes are measured by metrics tied to production delivery velocity, quality, and waste. DevOps emphasizes behavioral- or cultural-related changes such as those which encourage teaming, inclusion, feedback, and experimentation. Technological interventions such as automation are central as they can reinforce such target behaviors. DevOps does not necessarily imply functional roles in software delivery such as development, quality assurance, or operations are merged or seconded. More important is that a professional respect and shared sensibility is formed across the delivery team.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_containers&quot;&gt;Containers&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Containers are the runtime representation of a packaging format based on a lightweight, immutable image. Runtime dependencies are resolved within the image which facilitates portability. This makes possible the agreement on a standardized software work product. Management and runtime tooling that is container aware can then be applied consistently no matter what the underlying technology stack. Container-based workloads are suitable for multi-tenancy on a single compute instance and when implemented securely can realize significant operation efficiencies. An important corollary is that launching a new workload does not incur the cost of provisioning new compute infrastructure. This enables a true on-demand, self-service experience for users.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_container_orchestration&quot;&gt;Container Orchestration&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Container orchestration involves the lifecycle management of container workloads, including functions such as to schedule, stop, start, and replicate across a cluster of machines. Compute resources for running workloads are abstracted, allowing the host infrastructure to be treated as a single logical deployment target. Kubernetes is an open source community project addressing container orchestration. It groups containers that make up an application into logical units for easy management and discovery, and features self-healing, service discovery, load balancing, and storage services among its rich feature set. Orchestration plays a critical role in our design goal of application-centricity as quality of service attributes and deployment patterns are executed by invoking Kubernetes API primitives.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_continuous_integration&quot;&gt;Continuous Integration&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Continuous integration (CI) concerns the integration of code from potentially multiple authors into a shared source code management (SCM) repository. Such check-ins could occur many times a day, and automation steps in such a process could include gates or controls to expose any issues as early as possible. SCMs such as Git include¬†workflow support to commit to trunk, push, and merge code pull requests from multiple developers. With containers, a Git push event could be configured to then trigger an image build event via the webhooks mechanism.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_continuous_delivery&quot;&gt;Continuous Delivery&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once a CI strategy is in place, consideration can then move to achieving continuous delivery (CD). This involves automating the steps required to promote the work product from one environment to the next within the defined software development lifecycle (SDLC). Such steps could include automated testing, smoke, unit, functional, and static code analysis and static dependency checks for known security vulnerabilities. With containers, promotion in later stages of the SLC may merely involve the tagging of the (immutable) image to mark acceptance. Binary promotions are also possible such that only the image is pushed (to the target registry of the new environment), leaving source code in situ.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_continuous_deployment&quot;&gt;Continuous Deployment&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By convention, we can denote the special case of automated continuous delivery to production as &lt;em&gt;continuous deployment&lt;/em&gt; (CD). We make such a distinction because such deployments may be subject to additional governance processes and gates&amp;#x2014;for example, deliberate human intervention to manage risk and complete sign-off procedures. We make such a distinction because such deployments may be subject to additional governance processes. As per &lt;a href=&quot;#one_1&quot;&gt;Continuous delivery versus deployment&lt;/a&gt;, there may be scenarios for deliberate human intervention to manage risk and complete sign-off procedures.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;one_1&quot; class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/2022/11/dowo_0101.png&quot; alt=&quot;Continuous Delivery versus Deployment&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;title&quot;&gt;Figure 2. Continuous delivery versus deployment&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_pipelines&quot;&gt;Pipelines&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Pipelines are a representation of the flow/automation in a CI/CD process. Typically a pipeline might call out discrete steps in the software delivery process and present them visually or via a high-level scripting language so the flow can be manipulated. The steps might include build, unit tests, acceptance tests, packaging, documentation, reporting, and deployment and verification phases. Well-designed pipelines help deliver better quality code faster by enabling participants in the software delivery process to more easily diagnose and respond to feedback. As illustrated in &lt;a href=&quot;#one_2&quot;&gt;Smaller releases, release often, faster feedback&lt;/a&gt;, diagnosis and response turnaround can be accelerated by organizing releases into smaller and more frequent release bundles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;one_2&quot; class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/2022/11/dowo_0102.png&quot; alt=&quot;Smaller, more frequent releases&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;title&quot;&gt;Figure 3. Smaller releases, release often, faster feedback&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_software_configuration_management&quot;&gt;Software Configuration Management&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For our purposes we will take a narrower view of software configuration management (CM) and focus on the recommended software engineering practice of separating dynamic configuration from static runtime software. Doing so allows developers and operations engineers to change the configuration without having to rebuild the runtime such as might occur when deploying to different environments. Containers, based as they are on immutable images, amplify this behavior as the alternative would be configuration layered across multiple images for each deployment scenario.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_deployment_patterns&quot;&gt;Deployment Patterns&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Aligned with the goal of automation across all steps in the software delivery lifecycle are patterns for deployment. We look here for strategies that can balance across criteria including safety, testability, reversibility, and downtime minimization in cloud-scale scenarios. Some deployment patterns also offer opportunities for capturing and responding to feedback. An A/B deployment allows for testing a user-defined hypothesis such as whether application version A is more effective than B. Usage results can then drive weighted load balancing across the alternatives. Automation of deployment strategies in this DevOps world are implemented by driving the orchestration APIs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_continuous_improvement&quot;&gt;Continuous Improvement&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s conclude this chapter by covering continuous improvement (&lt;a href=&quot;#one_3&quot;&gt;Continuous improvement&lt;/a&gt;), which should be the thread that connects all of the process improvement&amp;#x2013;related practices summarized. The environment changes and so must we. These practices make it easy and inexpensive to experiment, formulate, and test hypotheses, as well as capture, act on, and experiment with the feedback received. This way we can continue to inject energy into the system and so maintain a state of dynamic stability&amp;#x2014;a balance of adaptive/agile versus fixed/stable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;one_3&quot; class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/2022/11/dowo_0103.png&quot; alt=&quot;Continuous Improvement&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;title&quot;&gt;Figure 4. Continuous improvement&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_summary&quot;&gt;Summary&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We covered here some of what is unique and nuanced about DevOps with OpenShift and why it matters. Realizing these DevOps concepts using natively container-aware automation can bring cloud deployment power to &lt;em&gt;all&lt;/em&gt; the people, from 10x programmer to citizen developer. The following chapters will show you how.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>SNO in AWS for $150/mo</title>
          <link>https://blog.eformat.me/2022/11/aws-sno-150.html</link>
          <pubDate>Thu, 10 Nov 2022 05:09:03 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/aws-sno-150.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_so_you_want_to_demo_openshift_like_a_boss&quot;&gt;So you want to demo OpenShift like a boss &amp;#8230;&amp;#8203;&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div id=&quot;money&quot; class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/2022/11/100-unsplash.jpg&quot; alt=&quot;Money&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What is the cheapest way to run OpenShift in the public cloud ?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Behold .. the awesomeness-ness of SNO (Single Node OpenShift) on persistent &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html&quot;&gt;spot in AWS&lt;/a&gt;. A Spot Instance is an instance that uses spare EC2 capacity that is available for a lot less than the On-Demand price. How much less ? well.. you can &lt;a href=&quot;https://aws.amazon.com/ec2/spot/pricing&quot;&gt;check it out here&lt;/a&gt; but normally 70% less ec2 cost. Just get used to some interruptions üò∂‚Äçüå´Ô∏è.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For installing and demoing &lt;em&gt;anything&lt;/em&gt; in OpenShift you will normally need a bare minimum of 8vCPU and 32 GB RAM for SNO which may get you close to under the $100 mark üò≤.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-ends grid-all&quot; style=&quot;width: 50%;&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 33.3333%;&quot;&gt;
&lt;col style=&quot;width: 33.3333%;&quot;&gt;
&lt;col style=&quot;width: 33.3334%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;/th&gt;
&lt;th class=&quot;tableblock halign-center valign-top&quot; colspan=&quot;2&quot;&gt;Price&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;m6a.2xlarge&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-center valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;$0.1658 per Hour&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;em&gt;$120&lt;/em&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;GP3 volumes&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-center valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;approx&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;em&gt;$10&lt;/em&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;ELB+EIP&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-center valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;approx&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;em&gt;$20&lt;/em&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;Total:&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-center valign-top&quot;&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;em&gt;$150&lt;/em&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But others could suit your need better:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;c5n.4xlarge - 16 vCPU, 42 GB RAM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;m6a.2xlarge - 8 vCPU, 32 GB RAM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;r6i.2xlarge - 8 vCPU, 64 GB RAM&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Prices will vary over time ! it is spot after all. The rate of instance &lt;a href=&quot;https://aws.amazon.com/ec2/spot/instance-advisor&quot;&gt;interruption&lt;/a&gt; also varies by region and instance type, so I pick and choose based on latency to where I work from.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, how do we get there ?&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_configuring_and_installing_openshift&quot;&gt;Configuring and Installing OpenShift&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can &lt;a href=&quot;https://docs.openshift.com/container-platform/4.11/installing/installing_sno/install-sno-installing-sno.html&quot;&gt;check the docs&lt;/a&gt; for configuring the install.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ openshift-install create install-config&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You want to install SNO, so your config should look similar to this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;apiVersion: v1
baseDomain: &amp;lt;your base domain&amp;gt;
compute:
- name: worker
  replicas: 0
controlPlane:
  name: master
  replicas: 1
  architecture: amd64
  hyperthreading: Enabled
  platform:
    aws:
      type: c5n.4xlarge
      rootVolume:
        size: 250
        type: gp3
metadata:
  name: sno
platform:
  aws:
    region: &amp;lt;your region&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You want a single master, choose how big you want your root volume and instance size and which region to install to. Personally I use &lt;a href=&quot;https://github.com/openshift/hive/blob/master/docs/clusterpools.md&quot;&gt;Hive and ClusterPools&lt;/a&gt; from an SNO instance in my home lab to install all my public cloud clusters, that way I can easily control then via configuration and &lt;a href=&quot;https://github.com/openshift/hive/blob/master/docs/hibernating-clusters.md&quot;&gt;hibernate&lt;/a&gt; them when I want ! You can also just install via the cli of course:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ openshift-install create cluster&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_adjusting_sno_to_remove_all_the_costly_networking_bits&quot;&gt;Adjusting SNO to remove all the costly networking bits!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When you install SNO, it installs a bunch of stuff you may not want in a demo/lab environment. With a single node, the load balancers and the private routing are usually not necessary at all. It&amp;#8217;s always possible to put the private routing and subnets back if you need to add workers later or just reinstall.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I am going to include the aws cli commands as guidance, they need a bit more polish to make them fully scriptable, but we&amp;#8217;re working on it ! This saves you approx~ $120/mo for the 3 NAT gateways, $40/mo for 2 API load balancers and $10/mo for 2 EIP&amp;#8217;s. I will keep the router ELB.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Update Master Security Group: Allow 6443 (TCP)&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;region=&amp;lt;your aws region&amp;gt;
instance_id=&amp;lt;your instance id&amp;gt;
master_sg_name=&amp;lt;your cluster&amp;gt;-sno-master-sg

sg_master=$(aws ec2 describe-security-groups \
  --region=${region} \
  --query &quot;SecurityGroups[].GroupId&quot; \
  --filters &quot;Name=vpc-id,Values=${vpc}&quot; \
  --filters &quot;Name=tag-value,Values=${master_sg_name}&quot; | jq -r .[0])

aws ec2 authorize-security-group-ingress \
--region=${region} \
--group-id ${sg_master} \
--ip-permissions &apos;[{&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: 6443, &quot;ToPort&quot;:6443, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]}]&apos;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update Master Security Group: Allow 30000 to 32767 (TCP &amp;amp; UDP) from 0.0.0.0/0 for NodePort services&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws ec2 authorize-security-group-ingress \
--region=${region} \
--group-id ${sg_master} \
--ip-permissions &apos;[{&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: 30000, &quot;ToPort&quot;:32767, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]},{&quot;IpProtocol&quot;: &quot;udp&quot;, &quot;FromPort&quot;: 30000, &quot;ToPort&quot;:32767, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]}]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add Security Groups that were attached to Routing ELB to master&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws ec2 authorize-security-group-ingress \
--region=${region} \
--group-id ${sg_master} \
--ip-permissions &apos;[{&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: 443, &quot;ToPort&quot;:443, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]},{&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: 80, &quot;ToPort&quot;:80, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]},{&quot;IpProtocol&quot;: &quot;icmp&quot;, &quot;FromPort&quot;: 8, &quot;ToPort&quot;: -1,&quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]}]&apos;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Attach a new public elastic IP address&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;eip=$(aws ec2 allocate-address --domain vpc --region=${region})

aws ec2 associate-address \
--region=${region} \
--allocation-id $(echo ${eip} | jq -r &apos;.AllocationId&apos;) \
--instance-id ${instance_id}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update all subnets to route through IGW (using public route table)&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;# update public route table and add private subnets to route through igw (using public route table), public subnets already route that way
aws ec2 describe-route-tables --filters &quot;Name=vpc-id,Values=${vpc}&quot; --region=${region} &amp;gt; /tmp/baz

# inspect /tmp/baz to get the right id&apos;s, update them individually
aws ec2 replace-route-table-association \
--association-id rtbassoc-&amp;lt;id&amp;gt; \
--route-table-id rtb-&amp;lt;id for igw&amp;gt; \
--region=${region}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Route53: Change API, APPS - A record to elastic IP address&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Route53: Change internal API, APPS - A records to private IP address of instance&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I&amp;#8217;m just going to list the generic command here, rinse and repeat for each of the zone records (four times, [int, ext] - for [*.apps and api]):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws route53 list-hosted-zones

# get your hosted zone id&apos;s
hosted_zone=/hostedzone/&amp;lt;zone id&amp;gt;

# use the private ip address for the internal zone
cat &amp;lt;&amp;lt; EOF &amp;gt; /tmp/route53_policy1
{
            &quot;Changes&quot;: [
              {
                &quot;Action&quot;: &quot;UPSERT&quot;,
                &quot;ResourceRecordSet&quot;: {
                  &quot;Name&quot;: &quot;api.&amp;lt;your cluster domain&amp;gt;&quot;,
                  &quot;Type&quot;: &quot;A&quot;,
                  &quot;TTL&quot;: 300,
                  &quot;ResourceRecords&quot;: [
                    {
                      &quot;Value&quot;: &quot;$(echo $eip | jq -r &apos;.PublicIp&apos;)&quot;
                    }
                  ]
                }
              }
            ]
          }
EOF

aws route53 change-resource-record-sets \
--region=${region} \
--hosted-zone-id $(echo ${hosted_zone} | sed &apos;s/\/hostedzone\///g&apos;) \
--change-batch file:///tmp/route53_policy1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete NAT gateways&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will delete all your nat gateways, adjust to suit&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;for i in `aws ec2 describe-nat-gateways --region=${region} --query=&quot;NatGateways[].NatGatewayId&quot; --output text | tr &apos;\n&apos; &apos; &apos;`; do aws ec2 delete-nat-gateway --nat-gateway-id ${i} --region=${region}; done&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Release public IP addresses (from NAT gateways)&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There will be two public EIP&amp;#8217;s you can now release:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws ec2 release-address \
--region=${region} \
--public-ip &amp;lt;public ip address&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete API load balancers (ext, int)&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will delete all your api load balancers, adjust to suit&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;for i in `aws elb describe-load-balancers --region=${region} --query=&quot;LoadBalancerDescriptions[].LoadBalancerName&quot; --output text | tr &apos;\n&apos; &apos; &apos;`; do aws elb delete-load-balancer --region=${region} --load-balancer-name ${i}; done&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete API load balancer target groups&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;FIXME - need to look these up&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws elbv2 delete-target-group \
--target-group-arn arn:aws:elasticloadbalancing:us-west-2:123456789012:targetgroup/my-targets/73e2d6bc24d8a067&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restart SNO to ensure it still works !&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_convert_sno_to_spot&quot;&gt;Convert SNO to SPOT&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This has the effect of creating a spot request which will be permanent and only stop the instance should the price or capacity not be met temporarily. We&amp;#8217;re using &lt;a href=&quot;https://pythonawesome.com/a-tool-to-convert-aws-ec2-instances-back-and-forth-between-on-demand&quot;&gt;this script&lt;/a&gt; to convert the SNO instance:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ ./ec2-spot-converter --stop-instance --review-conversion-result --instance-id &amp;lt;your instance id&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will take a bit of time to run and gives good debugging info. You can delete any temporary ami&amp;#8217;s and snapshots it creates.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_a_little_work_in_progress&quot;&gt;A little work in progress &amp;#8230;&amp;#8203;&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The script changes your instance id to a new one during the conversion. There is &lt;em&gt;one&lt;/em&gt; issue we have not yet got a proper workaround for. When the kubelet first registers itself as a node, the initial instance id is set in config in etcd. This causes the new instance to not properly associate with the router ELB. We can re-register the new id:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;# Check the router ELB to ensure the new instance is registered there.
# Re-attach the new instance - need to do each restart until workaround found !
aws elb register-instances-with-load-balancer \
--load-balancer-name &amp;lt;router elb id&amp;gt; \
--instances &amp;lt;your new converted instance id&amp;gt; \
--region=${region}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To make this survive a restart, we need to change the aws service provider id by hand on disk.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc debug node/&amp;lt;your node name&amp;gt;.compute.internal
chroot /host
cat /etc/systemd/system/kubelet.service.d/20-aws-providerid.conf

# the file will look like this with your region and instance
[Service]
Environment=&quot;KUBELET_PROVIDERID=aws:///&amp;lt;region&amp;gt;/&amp;lt;your original instance id&amp;gt;&quot;

# edit this file using vi and change &amp;lt;your original instance id&amp;gt; -&amp;gt; &amp;lt;your new converted instance id&amp;gt;

# restart the service
systemctl restart aws-kubelet-providerid.service

# restart SNO&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I will update this blog if we get a better way to manage this instance id thing over time ü§ûü§ûü§û&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_profit&quot;&gt;Profit !&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;üí∏üí∏üí∏ You should now be off to the races üèáüèª with your cheap-as SNO running on Spot.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The next steps - normally I would add a Lets Encrypt Cert, add users and configure the LVM Operator for thin-lvm based storage class. That i will leave those steps for another blog. Enjoy. ü§ë&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>Patterns with ArgoCD - Vault</title>
          <link>https://blog.eformat.me/2022/11/argocd-patterns-vault.html</link>
          <pubDate>Thu, 10 Nov 2022 05:09:03 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/argocd-patterns-vault.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_team_collaboration_with_argocd&quot;&gt;Team Collaboration with ArgoCD&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have written before about collaborating using GitOps, ArgoCD and Red Hat GitOps Operator. How can we better align our deployments with our teams ?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The array of patterns and the helm chart are described &lt;a href=&quot;https://github.com/redhat-cop/helm-charts/blob/master/charts/gitops-operator/TEAM_DOCS.md&quot;&gt;in a fair bit of detail here&lt;/a&gt;. I want to talk about using one of these patterns at scale - hundred&amp;#8217;s of apps across multiple clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_platform_cluster_argocd_tenant_team_argocds&quot;&gt;Platform Cluster ArgoCD, Tenant Team ArgoCD&amp;#8217;s&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For Product Teams working in large organisations that have a central Platform Team - this pattern is probably the most natural i think.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;sre-cluster-argo-team-namespaced&quot;&gt;
  &lt;img src=&quot;/2022/11/sre-cluster-argo-team-namespaced.png&quot; class=&quot;zoom&quot;&gt;
  &lt;div class=&quot;title&quot;&gt;Figure - Platform ArgoCD, Namespaced ArgoCD per Team&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It allows the platform team to control cluster and elevated privileges, activities like controlling namespaces, configuring cluster resources etc, in their Cluster Scoped ArgoCD, whilst Product Teams can control their namespaces independently of them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The RedHat GitOps Operator (cluster scoped)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Platform Team (cluster scoped) ArgoCD instance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Team (namespace scoped) ArgoCD instances&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When doing multi-cluster, i usually prefer to have ArgoCD &lt;em&gt;&quot;in the cluster&quot;&lt;/em&gt; rather than remotely controlling a cluster. This seems better from an availability / single point of failure point of view. Of course if its a more &lt;em&gt;edge&lt;/em&gt; use case, remote cluster connections may make sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OK, so the Tenant ArgoCD is deployed in &lt;strong&gt;namespaced&lt;/strong&gt; mode and controls multiple namespaces belonging to a team. For each Team, a single ArgoCD instance per cluster normally suffices. You can scale up and shard the argo controllers, run in HA - not usually necessary at team scale (100 apps) - see the argocd doco if you need to do this though. There may be multiple non-production clusters - dev, test, qa etc and then you will have multiple production clusters (prod + dr etc) - each cluster have their own ArgoCD instances per Team.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All of this is controlled via gitops. A sensible code split is one git repo per team, so one for the Platform Team, one for each Product Team - i normally start with a mono repo and split later based on need or scale.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is worth pointing out that any elevated cluster RBAC permissions that are needed by the Product Teams&apos; are done via git PR&amp;#8217;s into the platform team&amp;#8217;s gitops repo. Once configured, the Tenant team is in control of their namespaces and can get on with managing their own products and tooling.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_secrets_management_with_argocd_vault_plugin&quot;&gt;Secrets Management with ArgoCD Vault Plugin&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To make this work at scale and in production within an organisation, the &quot;batteries&quot; for secrets management must be included! They are table stakes really. It&amp;#8217;s fiddly, but worth the effort.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are many ways to do secrets management beyond k8s secrets - &lt;a href=&quot;https://cloud.redhat.com/blog/a-guide-to-gitops-and-secret-management-with-argocd-operator-and-sops&quot;&gt;KSOPS&lt;/a&gt;, &lt;a href=&quot;https://external-secrets.io&quot;&gt;External Secrets Operator&lt;/a&gt; etc. The method i want to talk about uses the &lt;a href=&quot;https://argocd-vault-plugin.readthedocs.io/en/stable/backends/&quot;&gt;ArgoCD Vault Plugin&lt;/a&gt; which i will abbreviate to &lt;strong&gt;AVP&lt;/strong&gt;. It supports multiple secret backends by the way. In this case, i am going to use &lt;a href=&quot;https://developer.hashicorp.com/vault/docs/auth/kubernetes&quot;&gt;Hashicorp Vault&lt;/a&gt; and the k8s integration auth method. Setting up vault is dealt with &lt;a href=&quot;https://eformat.github.io/vault-quickstart/&quot;&gt;separately&lt;/a&gt; but can be done on-cluster or off-cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To get AVP working, you basically deploy the ArgoCD repo server with a ServiceAccount and use that secret to authenticate to Hashi Vault using k8s auth method. This way each ArgoCD instance uses the token associated with that service account to authenticate. Note that in OpenShift 4.11+ when creating new service accounts (SA), a service account token secret is &lt;a href=&quot;https://docs.openshift.com/container-platform/4.11/nodes/pods/nodes-pods-secrets.html#nodes-pods-secrets-creating-sa_nodes-pods-secrets&quot;&gt;no longer automatically generated.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once done, our app secrets can be easily referenced from within source code using either annotations:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;kind: Secret
apiVersion: v1
metadata:
  name: example-secret
  annotations:
    avp.kubernetes.io/path: &quot;path/to/app-secret&quot;
type: Opaque
data:
  password: &amp;lt;password-vault-key&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;or directly via the full path:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;  password: &amp;lt;path:kv/data/path/to/app-secret#password-vault-key&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can also reference the secrets directly from our ArgoCD Application definitions. Here is an example of using helm (kustomize and plain yaml are also supported).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;  source:
    repoURL: https://github.com/eformat/my-gitrepo.git
    path: gitops/my-app/chart
    targetRevision: main
    plugin:
      name: argocd-vault-plugin-helm
      env:
        - name: HELM_VALUES
          value: |
            image:
              repository: image-registry.openshift-image-registry.svc:5000/my-namespace/my-app
              tag: &quot;1.2.3&quot;
            password: &amp;lt;path:kv/data/path/to/app-secret#password-vault-key&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I also use a pattern to pass the &lt;em&gt;vault annotation path&lt;/em&gt; down to the helm chart from the ArgoCD Application. To keep things clean (and you sane!) I normally have a Vault secret per-application (containing many KV2 - key:value pairs).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;    plugin:
      name: argocd-vault-plugin-helm
      env:
        - name: HELM_VALUES
          value: |
            resources:
              limits:
                cpu: 500m         # a non secret value
            avp:
              secretPath: &quot;kv/data/path/to/app-secret&quot;  # use this in the annotations&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This allows you to control the &lt;strong&gt;path&lt;/strong&gt; to your secrets in Vault which can be configured by convention e.g.  &lt;strong&gt;kv/data/cluster/namespace/app&lt;/strong&gt; as an example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_argocd_configuration_the_gory_details&quot;&gt;ArgoCD Configuration - The Gory Details&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OK, great. But how do i get there with my Team ArgoCD ? Let&amp;#8217;s take a look in depth at the &lt;a href=&quot;https://github.com/redhat-cop/helm-charts/blob/master/charts/gitops-operator/values.yaml&quot;&gt;&lt;strong&gt;argocd-values.yaml&lt;/strong&gt;&lt;/a&gt; file you might pass into the gitops-operator helm chart to bootstrap your ArgoCD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The important bit for AVP integration is to mount the token from a service account that we have created - in this case the service account is called &lt;strong&gt;argocd-repo-vault&lt;/strong&gt; and we set &lt;strong&gt;mountastoken&lt;/strong&gt; to &quot;true&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next, we use an &lt;strong&gt;initContainer&lt;/strong&gt; to download the AVP go binary and save it to a &lt;strong&gt;custom-tools&lt;/strong&gt; directory. If you are doing this disconnected, the binary needs to be made available offline.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;argocd_cr:
  statusBadgeEnabled: true
  repo:
    mountsatoken: true
    serviceaccount: argocd-repo-vault
    volumes:
    - name: custom-tools
      emptyDir: {}
    initContainers:
    - name: download-tools
      image: registry.access.redhat.com/ubi8/ubi-minimal:latest
      command: [sh, -c]
      env:
        - name: AVP_VERSION
          value: &quot;1.11.0&quot;
      args:
        - &amp;gt;-
          curl -Lo /tmp/argocd-vault-plugin https://github.com/argoproj-labs/argocd-vault-plugin/releases/download/v\${AVP_VERSION}/argocd-vault-plugin_\${AVP_VERSION}_linux_amd64 &amp;amp;&amp;amp; chmod +x /tmp/argocd-vault-plugin &amp;amp;&amp;amp; mv /tmp/argocd-vault-plugin /custom-tools/
      volumeMounts:
      - mountPath: /custom-tools
        name: custom-tools
    volumeMounts:
    - mountPath: /usr/local/bin/argocd-vault-plugin
      name: custom-tools
      subPath: argocd-vault-plugin&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We need to create the &lt;em&gt;glue&lt;/em&gt; between our ArgoCD Applications&apos; and how they call/use the AVP binary. This is done using the &lt;strong&gt;configManagementPlugins&lt;/strong&gt; stanza. Note we use three methods, one for plain YAML, one for helm charts, one for kustomize. The plugin &lt;strong&gt;name:&lt;/strong&gt; is what we reference from our ArgoCD Application.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;  configManagementPlugins: |
    - name: argocd-vault-plugin
      generate:
        command: [&quot;sh&quot;, &quot;-c&quot;]
        args: [&quot;argocd-vault-plugin -s team-ci-cd:team-avp-credentials generate ./&quot;]
    - name: argocd-vault-plugin-helm
      init:
        command: [sh, -c]
        args: [&quot;helm dependency build&quot;]
      generate:
        command: [&quot;bash&quot;, &quot;-c&quot;]
        args: [&apos;helm template &quot;$ARGOCD_APP_NAME&quot; -n &quot;$ARGOCD_APP_NAMESPACE&quot; -f &amp;lt;(echo &quot;$ARGOCD_ENV_HELM_VALUES&quot;) . | argocd-vault-plugin generate -s team-ci-cd:team-avp-credentials -&apos;]
    - name: argocd-vault-plugin-kustomize
      generate:
        command: [&quot;sh&quot;, &quot;-c&quot;]
        args: [&quot;kustomize build . | argocd-vault-plugin -s team-ci-cd:team-avp-credentials generate -&quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We make use of environment variables set within the AVP plugin for helm so that the namespace and helm values from the ArgoCD Application are set correctly. See the &lt;a href=&quot;https://argocd-vault-plugin.readthedocs.io/en/stable/usage/&quot;&gt;AVP documentation&lt;/a&gt; for full details of usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One thing to note, is the &lt;strong&gt;team-ci-cd:team-avp-credentials&lt;/strong&gt; secret. This specifies &lt;em&gt;how&lt;/em&gt; the AVP binary connects and authenticates to Hashi Vault. It is a secret that you need to set up. An example as follows for a simple hashi vault in-cluster deployment:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;export AVP_TYPE=vault
export VAULT_ADDR=https://vault-active.hashicorp.svc:8200   # vault url
export AVP_AUTH_TYPE=k8s                              # kubernetes auth
export AVP_K8S_ROLE=argocd-repo-vault                 # vault role (service account name)
export VAULT_SKIP_VERIFY=true
export AVP_MOUNT_PATH=auth/$BASE_DOMAIN-$PROJECT_NAME

cat &amp;lt;&amp;lt;EOF | oc apply -n ${PROJECT_NAME} -f-
---
apiVersion: v1
stringData:
  VAULT_ADDR: &quot;${VAULT_ADDR}&quot;
  VAULT_SKIP_VERIFY: &quot;${VAULT_SKIP_VERIFY}&quot;
  AVP_AUTH_TYPE: &quot;${AVP_AUTH_TYPE}&quot;
  AVP_K8S_ROLE: &quot;${AVP_K8S_ROLE}&quot;
  AVP_TYPE: &quot;${AVP_TYPE}&quot;
  AVP_K8S_MOUNT_PATH: &quot;${AVP_MOUNT_PATH}&quot;
kind: Secret
metadata:
  name: team-avp-credentials
  namespace: ${PROJECT_NAME}
type: Opaque
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I am leaving out the gory details of Vault/ACL setup which are documented &lt;a href=&quot;https://eformat.github.io/vault-quickstart/&quot;&gt;elsewhere&lt;/a&gt;, however to create the auth secret in vault from the &lt;strong&gt;argocd-repo-vault&lt;/strong&gt; ServiceAccount token, i use this shell script:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;export SA_TOKEN=$(oc -n ${PROJECT_NAME} get sa/${APP_NAME} -o yaml | grep ${APP_NAME}-token | awk &apos;{print $3}&apos;)
export SA_JWT_TOKEN=$(oc -n ${PROJECT_NAME} get secret $SA_TOKEN -o jsonpath=&quot;{.data.token}&quot; | base64 --decode; echo)
export SA_CA_CRT=$(oc -n ${PROJECT_NAME} get secret $SA_TOKEN -o jsonpath=&quot;{.data[&apos;ca\.crt&apos;]}&quot; | base64 --decode; echo)

vault write auth/$BASE_DOMAIN-${PROJECT_NAME}/config \
  token_reviewer_jwt=&quot;$SA_JWT_TOKEN&quot; \
  kubernetes_host=&quot;$(oc whoami --show-server)&quot; \
  kubernetes_ca_cert=&quot;$SA_CA_CRT&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_why_do_all_of_this&quot;&gt;Why Do All of This ?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The benefit of all this gory configuration stuff:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;we can now store secrets safely in a backend vault at enterprise scale&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we have all of our ArgoCD&amp;#8217;s use these secrets consistently with gitops in a multi-tenanted manner&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we keep secrets values out of our source code&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we can control all of this with gitops&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It also means that the platform an product teams, can manage secrets in a safely consistent manner - but separately i.e. each team manages their own secrets and space in vault. This method also works if you are using the enterprise Hashi vault that uses &lt;strong&gt;namespaces&lt;/strong&gt; - you can just set the env.var into your ArgoCD Application like so.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;    plugin:
      name: argocd-vault-plugin-kustomize
      env:
        - name: VAULT_NAMESPACE
          value: &quot;my-team-apps&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Tenant team&amp;#8217;s are now fully in control of their namespaces and secrets and can get on with managing their own applications, products and tools !&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>Pulsar Flink</title>
          <link>https://blog.eformat.me/2022/11/pulsar-flink.html</link>
          <pubDate>Wed, 2 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/pulsar-flink.html</guid>
          <description>
              &lt;h1&gt;&lt;a href=&quot;#pulsar-flink&quot; id=&quot;pulsar-flink&quot;&gt;Pulsar Flink&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;I have been messing around with yet another streaming demo (YASD). You really just cannot have too many. ü§©&lt;/p&gt;
&lt;p&gt;I am a fan of &lt;a href=&quot;https://en.wikipedia.org/wiki/Server-sent_events&quot;&gt;server sent events&lt;/a&gt;, why ? because they are HTML5 native. No messing around with web sockets. I have a a &lt;a href=&quot;https://github.com/eformat/quote-generator&quot;&gt;small quarkus app&lt;/a&gt; that generates stock quotes:&lt;/p&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/eformat/quote-generator/master/images/quotes.gif&quot; width=&quot;500&quot; class=&quot;zoom&quot;&gt;
&lt;p&gt;that you can easily run locally or on OpenShift:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;oc new-app quay.io/eformat/quote-generator:latest
oc create route edge quote-generator --service=quote-generator --port=8080
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then retrieve the events in the browser or by curl:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -H &amp;quot;Content-Type: application/json&amp;quot; --max-time 9999999 -N http://localhost:8080/quotes/stream
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, first challenge - How might we consume these SSE&amp;rsquo;s using Flink? I found a handy &lt;a href=&quot;https://github.com/aws-samples/amazon-kinesis-data-analytics-apache-flink-server-sent-events-sse/tree/main/kinesis-data-analytics-sse&quot;&gt;AWS Kinesis SSE demo&lt;/a&gt; which i snarfed the SSE/OKHttp code from. I wired this into flinks &lt;code&gt;RichSourceFunction&lt;/code&gt;:&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/4d5fd40d8566e99e866e1e7fd4cb6388.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;So now i could consume this SSE source as a &lt;code&gt;DataStream&lt;/code&gt;&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/c63c765710b00b9ce15201edd9aca87b.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;In the example, i wire in the stock quotes for &lt;code&gt;NFLX&lt;/code&gt; and &lt;code&gt;RHT&lt;/code&gt;. Next step, process these streams. Since i am new to flink, i started with a simple print function, then read this &lt;a href=&quot;https://flink.apache.org/news/2015/02/09/streaming-example.html&quot;&gt;stock price&lt;/a&gt; example from 2015! cool. So i implemented a simple &lt;code&gt;BuyFunction&lt;/code&gt; class that makes stock buy recommendations:&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/156cabbd95543e22f4faf90f9529a192.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;Lastly, it needs to be sent to a sink. Again, i started by using a simple print sink:&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/783e1d4a37bc33e91393416109a92b67.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;Friends of mine have been telling me how much more awesome &lt;code&gt;Pulsar&lt;/code&gt; is compared to &lt;code&gt;Kafka&lt;/code&gt; so i also tried out sending to a local pulsar container that you can run using:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;podman run -it -p 6650:6650 -p 8081:8080 --rm --name pulsar docker.io/apachepulsar/pulsar:2.10.2 bin/pulsar standalone
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And forwrded to pulsar using a simple utility class using the pulsar java client:&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/90121414185b9142d884b72cb1e7af1c.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;Then consume the messages to make sure they are there !&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;podman exec -i pulsar bin/pulsar-client consume -s my-subscription -n 0 persistent://public/default/orders
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And i need to write this post as well .. getting it to run in OpenShift &amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/11/flink-stocks-ocp.png&quot; width=&quot;640&quot; class=&quot;zoom&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;br /&gt;
Source code is here - &lt;a href=&quot;https://github.com/eformat/flink-stocks&quot;&gt;https://github.com/eformat/flink-stocks&lt;/a&gt;&lt;/p&gt;

          </description>
      </item>
      
      <item>
          <title>The Compelling Platform</title>
          <link>https://blog.eformat.me/2022/11/the-compelling-platform.html</link>
          <pubDate>Tue, 1 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/the-compelling-platform.html</guid>
          <description>
              &lt;h1&gt;&lt;a href=&quot;#the-compelling-platform&quot; id=&quot;the-compelling-platform&quot;&gt;The Compelling Platform&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;Build it and they will come !&amp;rdquo;&lt;/em&gt; - 1989 movie Field of Dreams.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;key&lt;/em&gt; ingredient for success when building a platform is that it must be &lt;em&gt;compelling&lt;/em&gt; to use. What makes a platform compelling ?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The platform is self-service for the overwhelming majority of use cases.&lt;/li&gt;
&lt;li&gt;The platform is composable, containing discrete services that can be used independently.&lt;/li&gt;
&lt;li&gt;The platform does not force an inflexible way of working upon the delivery team.&lt;/li&gt;
&lt;li&gt;The platform is quick and cheap to start using, with an easy on-ramp (e.g. Quick start guides, documentation, code samples)&lt;/li&gt;
&lt;li&gt;The platform has a rich internal user community for sharing&lt;/li&gt;
&lt;li&gt;The platform is secure and compliant by default&lt;/li&gt;
&lt;li&gt;The platform is up to date&lt;/li&gt;
&lt;li&gt;The platform is the thinnest viable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A platform should also be more than just software and APIs - it is documentation, and consulting, and support and evangelism, and templates and guidelines.&lt;/p&gt;
&lt;p&gt;You must also move away from &lt;em&gt;project&lt;/em&gt; as the primary mechanism for funding and staffing delivery of technology. Platform is a &lt;em&gt;product&lt;/em&gt;, and needs a long-lived and stable team tasked with both build and run.&lt;/p&gt;
&lt;h2&gt;&lt;a href=&quot;#define-platform&quot; id=&quot;define-platform&quot;&gt;Define Platform&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;A platform is a curated experience for the customer of the platform (engineers)&amp;rdquo;&lt;/em&gt; - Matthew Skelton.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;A digital platform is a foundation of self-service APIs, tools, services, knowledge and support which are arranged as a compelling internal product. Autonomous delivery teams can make use of the platform to deliver product features at a higher pace, with reduced co-ordination.&amp;rdquo;&lt;/em&gt; - Evan Botcher.&lt;/p&gt;
&lt;p&gt;What it is &lt;b&gt;NOT:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is not the limited virtualised hosting and locked-down centrally-managed tooling that you already have.&lt;/li&gt;
&lt;li&gt;It is not just OpenShift, Ansible or RHEL by themselves.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;fattest&lt;/em&gt; platform in the world.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The starting point is to &lt;em&gt;&amp;ldquo;Use these N services in these ways &amp;hellip;&amp;rdquo;&lt;/em&gt; - this is a curated experience.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Thinnest Viable Platform&lt;/em&gt; is a small, curated set of complementary services or patterns used together to simplify and accelerate delivery.&lt;/p&gt;
&lt;p&gt;The platform will evolve and its design should meet common team interaction modes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/eformat/the-compelling-platform/blob/main/PATTERNS.md&quot;&gt;Patterns for the Compelling Platform &amp;gt;&lt;/a&gt;&lt;/p&gt;

          </description>
      </item>
      

  </channel> 
</rss>
