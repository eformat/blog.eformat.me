<?xml version="1.0"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>eformat.me</title>
    <link>https://blog.eformat.me</link>
    <atom:link href="https://blog.eformat.me/feeds/posts/default.xml" rel="self" type="application/rss+xml" />
    <description>eformat.me</description>
    <language>en-gb</language>
      <pubDate>Thu, 13 Apr 2023 01:59:18 +0000</pubDate>
      <lastBuildDate>Thu, 13 Apr 2023 01:59:18 +0000</lastBuildDate>

      
      <item>
          <title>OpenShift Install, Semi-Connected Registries and Mirror by Digest Images</title>
          <link>https://blog.eformat.me/2023/04/disconnected-registries.html</link>
          <pubDate>Wed, 12 Apr 2023 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2023/04/disconnected-registries.html</guid>
          <description>
              &lt;div id=&quot;preamble&quot;&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have been working with disconnected OpenShift clusters quite a lot recently. One of the things you need to deal with is disconnected registries and mirror by digest images.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_quay_transparent_proxy_pull_through_cache&quot;&gt;Quay Transparent Proxy-Pull Through Cache&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are a couple general approaches to configuring registries when disconnected. The &lt;a href=&quot;https://docs.openshift.com/container-platform/4.12/installing/disconnected_install/index.html&quot;&gt;product documentation&lt;/a&gt; has great depth of detail about using a Quay Mirror Registry. This is the right approach when wanting disconnected. The downside when you are testing things out in a lab is the mirror import process is both time-consuming and uses a lot of disk space.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One approach i have become fond of is a what i call a &lt;code&gt;semi-connected&lt;/code&gt; method, where your clusters&apos; use a &lt;a href=&quot;https://www.youtube.com/watch?v=oVlRDuCD6ic&quot;&gt;Quay Transparent Proxy-Pull Through Cache&lt;/a&gt; to speed things up. This still uses disk space, but you don&amp;#8217;t need to import all the images before installing a cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;After you install the quay mirror registry on the provisioning host, set this in your &lt;code&gt;config.yaml&lt;/code&gt; and restart the quay pods or service:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;FEATURE_PROXY_CACHE: true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This setup mimics what you would need to do when disconnected i.e. we always pull from the mirror registry when installing - but it is quicker to test as the mirror registry is connected. When configuring the OpenShift install method, the pull secret i use is &lt;strong&gt;just&lt;/strong&gt; to the mirror. More on that below.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you also set the cache timeout for your Organisations to be months or even years! then your images will hang around for a long time.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For installing OpenShift, you really need (at a minimum) two mirror organisations. I set up these two (admin is a default):&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;quay-mirror-orgs&quot;&gt;
  &lt;img src=&quot;/2023/04/quay-mirror-orgs.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Where each Organisation points to these registries:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;registry-redhat-io -&amp;gt; registry.redhat.io
ocp4-mirror -&amp;gt; quay.io/openshift-release-dev&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One nice trick is that you can &lt;strong&gt;base64 decode&lt;/strong&gt; your Red Hat &lt;strong&gt;pull-secret&lt;/strong&gt; (you download this from cloud.redhat.com) and use those credentials in the Organisation mirror registry setup for authentication.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_ocp_install_configuration&quot;&gt;OCP Install Configuration&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now comes for the tricky part - configuring your OpenShift installer setup. There are a several ways to do this. The one you use depends on your install method and how you wish to control the &lt;strong&gt;registries.conf&lt;/strong&gt; that gets configured for you cluster nodes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have been working with the &lt;strong&gt;Agent-based&lt;/strong&gt; installer method for Bare Metal (i fake it on libvirt with sushy) - you can &lt;a href=&quot;https://github.com/eformat/acm-gitops-ocp&quot;&gt;check out all the code here&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The issue i think everyone quickly discovers is that the OpenShift installer sets all mirror&amp;#8217;s by digest to be true i.e. &lt;strong&gt;mirror-by-digest-only = true&lt;/strong&gt;. If you check the &lt;a href=&quot;https://github.com/openshift/installer&quot;&gt;installer code&lt;/a&gt; its here:&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;ocp-installer-boostrap&quot;&gt;
  &lt;img src=&quot;/2023/04/ocp-installer-boostrap.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Setting mirror by digest to true is intentional, it helps stop image spoofing or getting an image from a moving tag.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Unfortunately not all Operators pull by digest either. In fact the deployments that are part of the &lt;strong&gt;openshift-marketplace&lt;/strong&gt; do not. So after a cluster install we see Image Pull errors like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ oc get pods -n openshift-marketplace
NAME                                   READY   STATUS             RESTARTS      AGE
certified-operators-d2nd9              0/1     ImagePullBackOff   0             15h
certified-operators-pqrlz              0/1     ImagePullBackOff   0             15h
community-operators-7kpbm              0/1     ImagePullBackOff   0             15h
community-operators-k662l              0/1     ImagePullBackOff   0             15h
marketplace-operator-84457bfc9-v22db   1/1     Running            4 (15h ago)   16h
redhat-marketplace-kjrt9               0/1     ImagePullBackOff   0             15h
redhat-marketplace-sqch2               0/1     ImagePullBackOff   0             15h
redhat-operators-4m4gt                 0/1     ImagePullBackOff   0             15h
redhat-operators-62z6x                 0/1     ImagePullBackOff   0             15h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And checking one of the pods we see it is trying to pull by tag:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ oc describe pod certified-operators-d2nd9
Normal  BackOff  2m2s (x4179 over 15h)  kubelet  Back-off pulling image &quot;registry.redhat.io/redhat/certified-operator-index:v4.12&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Unfortunately you cannot configure &lt;strong&gt;ImageContentSourcePolicy&lt;/strong&gt; for &lt;strong&gt;mirror-by-digest-only = false&lt;/strong&gt; so (currently) the only solution is to apply MachineConfig &lt;strong&gt;post&lt;/strong&gt; your install as a day#2 thing as documented in this &lt;a href=&quot;https://access.redhat.com/solutions/4817401&quot;&gt;Knowledge Base Article&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Hopefully in an upcoming OpenShift relaease (4.13 or 4.14) we will be able to use the &lt;strong&gt;new API&amp;#8217;s for CRDs ImageDigestMirrorSet ImageTagMirrorSet&lt;/strong&gt; - see &lt;a href=&quot;https://issues.redhat.com/browse/OCPNODE-521&quot;&gt;Allow mirroring images by tags&lt;/a&gt; RFE for more details on these changes.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For now though, i use &lt;strong&gt;butane&lt;/strong&gt; and MachineConfig as per the KB article at post install time to configure &lt;strong&gt;mirror-by-digest-only = false&lt;/strong&gt; for my mirror registries that need it. From my &lt;a href=&quot;https://github.com/eformat/acm-gitops-ocp&quot;&gt;git repo&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;butane 99-master-mirror-by-digest-registries.bu -o 99-master-mirror-by-digest-registries.yaml
oc apply -f 99-master-mirror-by-digest-registries.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will reboot your nodes to apply the MCP, you may add or change the butane template(s) and yaml to suit the nodes you need to target e.g. masters or workers (or any other) node role. In my case it&amp;#8217;s targeting a SNO cluster so master is fine.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All going well your marketplace pods should now pull images and run OK&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ oc get pods -n openshift-marketplace
NAME                                   READY   STATUS    RESTARTS   AGE
certified-operators-d2nd9              1/1     Running   0          16h
community-operators-k662l              1/1     Running   0          16h
marketplace-operator-84457bfc9-v22db   1/1     Running   5          16h
redhat-marketplace-kjrt9               1/1     Running   0          16h
redhat-operators-62z6x                 1/1     Running   0          16h&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A word of warning when using the Assited Installer / Agent Installer method. If you try to set &lt;strong&gt;mirror-by-digest-only = false&lt;/strong&gt; registries in your &lt;strong&gt;AgentServiceConfig&lt;/strong&gt; using the provided ConfigMap e.g. something like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;apiVersion: v1
kind: ConfigMap
metadata:
  name: quay-mirror-config
  namespace: multicluster-engine
  labels:
    app: assisted-service
data:
  LOG_LEVEL: &quot;debug&quot;
  ca-bundle.crt: |
    -----BEGIN CERTIFICATE-----
    ! Put you CA for your mirror registry here !
    -----END CERTIFICATE-----

  registries.conf: |
    unqualified-search-registries = [&quot;registry.redhat.io&quot;, &quot;registry.access.redhat.com&quot;, &quot;docker.io&quot;]

    [[registry]]
      prefix = &quot;&quot;
      location = &quot;registry.redhat.io/redhat&quot;
      mirror-by-digest-only = false
      [[registry.mirror]]
        location = &quot;quay.eformat.me:8443/registry-redhat-io/redhat&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The registry mirror setting will get reset to &lt;strong&gt;mirror-by-digest-only = true&lt;/strong&gt; by the installer.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Similarly, if you try and set MachineConfig in the &lt;strong&gt;ignitionConfigOverride&lt;/strong&gt; in the &lt;strong&gt;InfraEnv&lt;/strong&gt; e.g.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;apiVersion: agent-install.openshift.io/v1beta1
kind: InfraEnv
...
  # User for modify ignition during discovery
  ignitionConfigOverride: &apos;{&quot;ignition&quot;: {&quot;version&quot;: &quot;3.1.0&quot;}, &quot;storage&quot;: {&quot;files&quot;: [{&quot;path&quot;: &quot;/etc/containers/registries.conf&quot;, &quot;mode&quot;: 420, &quot;overwrite&quot;: true, &quot;user&quot;: { &quot;name&quot;: &quot;root&quot;},&quot;contents&quot;: {&quot;source&quot;: &quot;data:text/plain;base64,dW5xd...&quot;}}]}}&apos;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;it also gets overriden by the installer. I tried both these methods and failed 😭😭&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_summary&quot;&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For now, the only way to configure &lt;strong&gt;mirror-by-digest-only = false&lt;/strong&gt; is via MachineConfig &lt;strong&gt;post-install&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can always &lt;strong&gt;try&lt;/strong&gt; and only mirror images by digest, just remember that various operators and components may not be configured this work this way.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The future looks bright with the new API&amp;#8217;s, as this has been a long-standing issue now.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;🏅Good luck installing out there !!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>ACM &amp; ArgoCD for Teams</title>
          <link>https://blog.eformat.me/2023/02/acm-team-argocd.html</link>
          <pubDate>Fri, 17 Feb 2023 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2023/02/acm-team-argocd.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_quickly_deploying_argocd_applicationsets_using_rhacms_global_clusterset&quot;&gt;Quickly deploying ArgoCD ApplicationSets using RHACM&amp;#8217;s Global ClusterSet&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;gpu-concurrency-mechanisms&quot;&gt;
  &lt;img src=&quot;/2023/02/sre-cluster-argo-team-namespaced.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have &lt;a href=&quot;https://github.com/eformat/argocd-team-topologies&quot;&gt;written about&lt;/a&gt; how we can align our Tech to setup GitOps tooling so that it fits with our team structure.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;How can we make these patterns real using tools like Advanced Cluster Manager (ACM) that help us deploy to a fleet of Clusters ? ACM supports &lt;code&gt;Policy&lt;/code&gt; based deployments so we can track compliance of our clusters to the expected configuration management policy.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The source code is here - &lt;a href=&quot;https://github.com/eformat/acm-gitops&quot; class=&quot;bare&quot;&gt;https://github.com/eformat/acm-gitops&lt;/a&gt; - git clone it so you can follow along.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_global_clustersets&quot;&gt;Global ClusterSet&amp;#8217;s&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When a cluster is managed in ACM there are several resources created out of the box &lt;a href=&quot;https://access.redhat.com/documentation/en-us/red_hat_advanced_cluster_management_for_kubernetes/2.6/html-single/multicluster_engine/index#managedclustersets_global&quot;&gt;you can read about them here&lt;/a&gt; in the documentation. This includes a namespace called &lt;code&gt;open-cluster-management-global-set&lt;/code&gt;. We can quickly deploy &lt;code&gt;ApplicationSet&amp;#8217;s&lt;/code&gt; in this global-namespace that generates &lt;code&gt;Policy&lt;/code&gt; to create our team based ArgoCD instances.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can leverage the fact that &lt;code&gt;ApplicationSet&amp;#8217;s&lt;/code&gt; can be associated with a &lt;code&gt;Placement&lt;/code&gt; - that way we can easily control where our &lt;code&gt;Policy&lt;/code&gt; and &lt;code&gt;Team ArgoCD&amp;#8217;s&lt;/code&gt; are deployed across our fleet of OpenShift clusters by using simple label selectors for example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_bootstrap_a_cluster_scoped_argocd_for_our_policies&quot;&gt;Bootstrap a Cluster Scoped ArgoCD for our Policies&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We are going Bootstrap a cluster-scoped ArgoCD instance into the &lt;code&gt;open-cluster-management-global-set&lt;/code&gt; namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We will deploy our Team ArgoCD&amp;#8217;s using ACM &lt;code&gt;Policy&lt;/code&gt; that is generated using the &lt;code&gt;PolicyGenerator&lt;/code&gt; tool &lt;a href=&quot;https://github.com/stolostron/policy-generator-plugin/blob/main/docs/policygenerator-reference.yaml&quot;&gt;which you can read about here from its&apos; reference file&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Make sure to label the cluster&amp;#8217;s where you want to deploy to with &lt;code&gt;useglobal=true&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f bootstrap-acm-global-gitops/setup.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This deploys the following resources:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Subscription&lt;/code&gt; Resource - The GitOps operator &lt;code&gt;Subscription&lt;/code&gt;, including disabling the default ArgoCD and setting cluster-scoped connections for our namespaces - see the &lt;code&gt;ARGOCD_CLUSTER_CONFIG_NAMESPACES&lt;/code&gt; env.var that is part of the &lt;code&gt;Subscription&lt;/code&gt; object. If your namespace is not added here, you will get namespace scoped connections for your ArgoCD, rather than all namespaces.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;GitOpsCluster&lt;/code&gt; Resource - This resource provides a Connection between ArgoCD-Server and the Placement (where to deploy exactly the Application).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;Placement&lt;/code&gt; Resource - We use a &lt;code&gt;Placement&lt;/code&gt; resource for this global ArgoCD which deploys to a fleet of Clusters, where the Clusters needs to be labeled with &lt;code&gt;useglobal=true&lt;/code&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;code&gt;ArgoCD&lt;/code&gt; Resource - The CR for our global ArgoCD where we will deploy Policy. We configure ArgoCD to download the &lt;code&gt;PolicyGenerator&lt;/code&gt; binary, and configure kustomize to run with the setting:&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;kustomizeBuildOptions: --enable-alpha-plugins&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_deploy_the_team_based_argocd_using_generated_policy&quot;&gt;Deploy the Team Based ArgoCD using Generated Policy&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We are going to deploy ArgoCD for two teams now using the ACM &lt;code&gt;PolicyGenerator&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The &lt;code&gt;PolicyGenerator&lt;/code&gt; runs using kustomize. We specify the &lt;code&gt;generator-input/&lt;/code&gt; folder - that holds our YAML manifests for each ArgoCD - in this case one for &lt;code&gt;fteam&lt;/code&gt;, one for &lt;code&gt;zteam&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can run the &lt;code&gt;PolicyGenerator&lt;/code&gt; from the CLI to test it out before deploying - download it using the &lt;a href=&quot;https://github.com/stolostron/policy-generator-plugin/blob/main/README.md)&quot;&gt;instructions here&lt;/a&gt; e.g.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;kustomize build --enable-alpha-plugins team-gitops-policy/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We specify the placement rule &lt;code&gt;placement-team-argo&lt;/code&gt; - where the Clusters needs to be labeled with &lt;code&gt;teamargo=true&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We add some default compliance and control labels for grouping purposes in ACM Governance.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We also set the &lt;code&gt;pruneObjectBehavior: &quot;DeleteAll&lt;/code&gt; so that if we delete the &lt;code&gt;ApplicationSet&lt;/code&gt; the generated &lt;code&gt;Policy&lt;/code&gt; s deleted and all objects are removed. For this to work, we must also set the &lt;code&gt;remediationAction&lt;/code&gt; to &lt;code&gt;enforce&lt;/code&gt; for our Policies.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One last configuration is to set the ArgoCD &lt;code&gt;IgnoreExtraneous&lt;/code&gt; compare option - as Policy is generated we do not want ArgoCD to be out of sync for these objects.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;apiVersion: policy.open-cluster-management.io/v1
kind: PolicyGenerator
metadata:
  name: argocd-teams
placementBindingDefaults:
  name: argocd-teams
policyDefaults:
  placement:
    placementName: placement-team-argo
  categories:
    - CM Configuration Management
  complianceType: &quot;musthave&quot;
  controls:
    - CM-2 Baseline Configuration
  consolidateManifests: false
  disabled: false
  namespace: open-cluster-management-global-set
  pruneObjectBehavior: &quot;DeleteAll&quot;
  remediationAction: enforce
  severity: medium
  standards:
    - generic
  policyAnnotations: {&quot;argocd.argoproj.io/compare-options&quot;: &quot;IgnoreExtraneous&quot;}
policies:
  - name: team-gitops
    manifests:
      - path: generator-input/&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Make sure to label the cluster&amp;#8217;s where you want to deploy to with &lt;code&gt;teamargo=true&lt;/code&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To create our Team ArgoCD&amp;#8217;s run:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f applicationsets/team-argo-appset.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To delete them, remove the &lt;code&gt;AppSet&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc delete appset team-argo&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_summary&quot;&gt;Summary&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can now take this pattern and deploy it across multiple clusters that are managed by ACM. You can easily scale out the number of Team Based ArgoCD and have fine grained control over their individual configuration including third party plugins like Vault. ACM offers a single plane of glass to check if your clusters are compliant to the generated policies, and if not - take remedial action.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can see the code in action in this video.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;videoblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;iframe width=&quot;800&quot; height=&quot;600&quot; src=&quot;https://www.youtube.com/embed/eGxPMkADAbc?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;🏅Enjoy !!&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>SNO, MetalLB, BGP</title>
          <link>https://blog.eformat.me/2023/02/sno-metallb-bpg.html</link>
          <pubDate>Thu, 2 Feb 2023 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2023/02/sno-metallb-bpg.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_using_sno_and_metallb_in_bgp_mode&quot;&gt;Using SNO and MetalLB in BGP Mode&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So yeah, i was reading &lt;a href=&quot;https://cloud.redhat.com/blog/metallb-in-bgp-mode&quot;&gt;this awesome blog post on &apos;How to Use MetalLB in BGP Mode&apos;&lt;/a&gt; and thought i need to give this a try with SNO at home.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I won&amp;#8217;t repeat all the details linked in that post, please go read it before trying what comes next as i reference it. Suffice to say the following:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;SNO&lt;/strong&gt; - Single Node OpenShift&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;MetalLB&lt;/strong&gt; - creates &lt;code&gt;LoadBalancer&lt;/code&gt; types of Kubernetes services on top of a bare-metal (like) OpenShift/Kubernetes. I&amp;#8217;m going to do it in a kvm/libvirt lab.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;BGP&lt;/strong&gt; - Border Gateway Protocol - runs and scales the internet - (ftw! seriously, go read about bpg hijacking) - with MetalLB we can use BGP mode to statelessly load balance client traffic towards the applications running on bare metal-like OpenShift clusters.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The idea is that you can have both normal Routing/HAProxy service &lt;code&gt;ClusterIP&amp;#8217;s&lt;/code&gt; on the SDN as well as &lt;code&gt;LoadBalancer&amp;#8217;s&lt;/code&gt; being served by BGP/MetalLB in your SNO Cluster. OpenShift SDN (OVNKubernetes as well as OPenShiftSDN) both support MetalLB out of the box.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_the_lab_setup&quot;&gt;The Lab Setup&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_networking_services&quot;&gt;Networking Services&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are some complexities in my home lab, mainly caused by the constraint of having teenagers who feel good bandwidth is a basic human &lt;em&gt;right&lt;/em&gt; and not a &lt;em&gt;luxury&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So i need to keep the connections to their myriad of devices running smoothly as well as serving my own geek needs. To make this happen and keep things relatively simple, i have a pretty standard setup and use my Mesh network. I am not trying any telco grade stuff (e.g. SRIOV) - so have no main Cisco/vendor switching involved.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;gpu-concurrency-mechanisms&quot;&gt;
  &lt;img src=&quot;/2023/02/lab-network.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Router&lt;/strong&gt; - Plain old broadband router with firewall and port-forwarding facilities.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Mesh Network&lt;/strong&gt; - Connectivity via Wifi Mesh, 1G Ethernet around the house, comes with another firewall and port-forwarding facilities.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;VMHost&lt;/strong&gt; - Fedora Core box running libvirt/kvm. Has thin-lvm, nvme based storage. Hosts DNS, HTTPD, HAProxy services. Multiple network connections including eht0 which is bridged directly to the lab hosts via br0. When you add the virsh network, also make sure to &lt;a href=&quot;https://wiki.libvirt.org/page/Net.bridge.bridge-nf-call_and_sysctl.conf&quot;&gt;change the defaults for bridge mode to&lt;/a&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;cat /etc/sysctl.d/99-netfilter-bridge.conf
net.bridge.bridge-nf-call-ip6tables = 0
net.bridge.bridge-nf-call-iptables = 0
net.bridge.bridge-nf-call-arptables = 0

cat /etc/modules-load.d/br_netfilter.conf
br_netfilter

sudo sysctl -p /etc/sysctl.d/99-netfilter-bridge.conf&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And the bridge looks like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;cat &amp;lt;&amp;lt;EOF &amp;gt; /etc/libvirt/qemu/networks/sno.xml
&amp;lt;network&amp;gt;
  &amp;lt;name&amp;gt;sno&amp;lt;/name&amp;gt;
  &amp;lt;uuid&amp;gt;fc43091f-de22-4bf5-974b-98711b9f3d9e&amp;lt;/uuid&amp;gt;
  &amp;lt;forward mode=&quot;bridge&quot;/&amp;gt;
  &amp;lt;bridge name=&apos;br0&apos;/&amp;gt;
&amp;lt;/network&amp;gt;
EOF

virsh net-define /etc/libvirt/qemu/networks/sno.xml
virsh net-start sno
virsh net-autostart sno&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you have a firewall on this host (firewalld, iptables) make sure to allow these ports and traffic to flow: 179/TCP (BGP), 3784/UDP and 3785/UDP (BFD).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;SNO&lt;/strong&gt; - Single Node OpenShift 4.12 libvirt/kvm installed &lt;a href=&quot;https://github.com/eformat/ocp4-sno-inplace&quot;&gt;using libvirt Bootstrap In-Place methodology&lt;/a&gt; from a single iso. A snippet from my install-config file showing the networking setup.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; install-config.yaml
...
networking:
  networkType: OVNKubernetes
  machineNetwork:
  - cidr: 192.168.86.0/24&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When doing boostrap in-place, normally you rely on DHCP assignment for hostname, ip, dns, gateway. However, due to my DHCP being mesh controlled i modified the installer ISO to setup the networking manually. Set up so we copy the network form the boostrap image:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;cat &amp;lt;&amp;lt; EOF &amp;gt; install-config.yaml
...
bootstrapInPlace:
  installationDisk: &quot;--copy-network /dev/vda&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Setup the ip address, gateway, network, hostname, device, dns as &lt;a href=&quot;https://docs.openshift.com/container-platform/4.12/installing/installing_bare_metal/installing-restricted-networks-bare-metal.html#installation-user-infra-machines-advanced_installing-bare-metal&quot;&gt;per the OpenShift docs.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;arg1=&quot;rd.neednet=1&quot;
arg2=&quot;ip=192.168.86.32::192.168.86.1:255.255.255.0:sno:enp1s0:none nameserver=192.168.86.27&quot;
coreos-installer iso customize rhcos-live.x86_64.iso --live-karg-append=&quot;${arg1}&quot; --live-karg-append=&quot;${arg2}&quot; -f&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;DNS&lt;/strong&gt; - I run bind/named on my VMHost to control OpenShift api.* and apps.* cluster domain. The SOA is in the cloud, so I can route from anywhere to the FQDN OK. In the lab, the internal DNS server just gives you the lab IP address. Externally you are forwarded to the Router which port-forwards via the firewall&amp;#8217;s and Mesh to the correct SNO instance. I don&amp;#8217;t show it, but I run HAProxy on the VMHost - that way I can serve external traffic to multiple OpenShift clusters in the lab simultaneously. My DNS zone looks like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;ns1       IN     A       192.168.86.27
api       IN     A       192.168.86.32
api-int   IN     A       192.168.86.32
*.apps    IN     A       192.168.86.32&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;DHCP&lt;/strong&gt; - One of the drawback&amp;#8217;s of my mesh tech is that it does not allow you to override DNS on a per host / DHCP assigned basis. This is required to setup OpenShift (need control over DNS etc). I could have installed another DHCP server on linux to do this job, but I just figured &quot;no need&quot;, I will stick with the mesh as DHCP provider (see SNO section above for manual networking configuration).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;BGP&lt;/strong&gt; - Once installed, the bpg network looks like this.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;gpu-concurrency-mechanisms&quot;&gt;
  &lt;img src=&quot;/2023/02/bgp-lab-network.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When creating &lt;code&gt;LoadBalancer&lt;/code&gt; services in SNO, MetalLB with the help of FRR binds an &lt;code&gt;External IP&lt;/code&gt; to the service. Since we only have one SNO node, &lt;code&gt;BFD&lt;/code&gt; is not in use like the article (multiple worker nodes as &lt;code&gt;BGPPeer&amp;#8217;s&lt;/code&gt;). That&amp;#8217;s OK though we are just trying it out here!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;A nice addition for demoing, is being able to configure a &lt;a href=&quot;https://bird.network.cz&quot;&gt;Bird&lt;/a&gt; daemon on my Fedora Core laptop so that any BGP announcements are automatically added to its routing setup.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;FRR&lt;/strong&gt; - RHEL8 VM running &lt;a href=&quot;https://frrouting.org&quot;&gt;FRRouting (FRR)&lt;/a&gt; as a pod - this is an open source Internet routing protocol suite for Linux and Unix platforms. The configuration i used is from the linked blog post at the top. From the blog use the same &lt;code&gt;vtysh.conf&lt;/code&gt; and &lt;code&gt;daemons&lt;/code&gt; files. My &lt;code&gt;frr.conf&lt;/code&gt; files was as folows - i added an additional entry for my Bird Client BGPPeer at &lt;em&gt;192.168.86.109&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;cat &amp;lt;&amp;lt;&apos;EOF&apos; &amp;gt; /root/frr/frr.conf
frr version master_git
frr defaults traditional
hostname frr-upstream
!
debug bgp updates
debug bgp neighbor
debug zebra nht
debug bgp nht
debug bfd peer
log file /tmp/frr.log debugging
log timestamp precision 3
!
interface eth0
 ip address 192.168.86.23/24
!
router bgp 64521
 bgp router-id 192.168.86.23
 timers bgp 3 15
 no bgp ebgp-requires-policy
 no bgp default ipv4-unicast
 no bgp network import-check
 neighbor metallb peer-group
 neighbor metallb remote-as 64520
 neighbor 192.168.86.32 peer-group metallb
 neighbor 192.168.86.32 bfd
 neighbor 192.168.86.109 remote-as external
!
 address-family ipv4 unicast
  neighbor 192.168.86.32 next-hop-self
  neighbor 192.168.86.32 activate
  neighbor 192.168.86.109 next-hop-self
  neighbor 192.168.86.109 activate
 exit-address-family
!
line vty
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Running FRR with podman is pretty straight forward:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;podman run -d --rm  -v /root/frr:/etc/frr:Z --net=host --name frr-upstream --privileged quay.io/frrouting/frr:master&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Some useful commands i found to show you the BGP/FRR details:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;podman exec -it frr-upstream vtysh -c &quot;show ip route&quot;
podman exec -it frr-upstream ip r
podman exec -it frr-upstream vtysh -c &quot;show ip bgp sum&quot;
podman exec -it frr-upstream vtysh -c &quot;show ip bgp&quot;
podman exec -it frr-upstream vtysh -c &quot;show bfd peers&quot;
podman exec -it frr-upstream vtysh -c &quot;show bgp summary&quot;
podman exec -it frr-upstream vtysh -c &quot;show ip bgp neighbor&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;As in the blog post, when looking at your &quot;show ip bgp neighbor&quot; you should see &lt;strong&gt;BGP state = Established&lt;/strong&gt; for the &lt;code&gt;BGPPeers&lt;/code&gt; once everything is connected up.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;MetalLB&lt;/strong&gt; - Installed on SNO as per the blog post. Check there for a detailed explanation. The commands I used were as follows:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
---
apiVersion: v1
kind: Namespace
metadata:
name: metallb-system
spec: {}
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
---
apiVersion: operators.coreos.com/v1
kind: OperatorGroup
metadata:
name: metallb-operator
namespace: metallb-system
spec: {}
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
---
apiVersion: operators.coreos.com/v1alpha1
kind: Subscription
metadata:
name: metallb-operator-sub
namespace: metallb-system
spec:
name: metallb-operator
channel: &quot;stable&quot;
source: redhat-operators
sourceNamespace: openshift-marketplace
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc get installplan -n metallb-system
oc get csv -n metallb-system -o custom-columns=&apos;NAME:.metadata.name, VERSION:.spec.version, PHASE:.status.phase&apos;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
---
apiVersion: metallb.io/v1beta1
kind: MetalLB
metadata:
name: metallb
namespace: metallb-system
spec:
nodeSelector:
node-role.kubernetes.io/worker: &quot;&quot;
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
---
apiVersion: metallb.io/v1beta1
kind: IPAddressPool
metadata:
name: address-pool-bgp
namespace: metallb-system
spec:
addresses:
- 192.168.155.150/32
- 192.168.155.151/32
- 192.168.155.152/32
- 192.168.155.153/32
- 192.168.155.154/32
- 192.168.155.155/32
autoAssign: true
protocol: bgp
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
---
apiVersion: metallb.io/v1beta1
kind: BFDProfile
metadata:
name: test-bfd-prof
namespace: metallb-system
spec:
detectMultiplier: 37
echoMode: true
minimumTtl: 10
passiveMode: true
receiveInterval: 35
transmitInterval: 35
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
---
apiVersion: metallb.io/v1beta1
kind: BGPPeer
metadata:
name: peer-test
namespace: metallb-system
spec:
bfdProfile: test-bfd-prof
myASN: 64520
peerASN: 64521
peerAddress: 192.168.86.23
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
apiVersion: metallb.io/v1beta1
kind: BGPAdvertisement
metadata:
name: announce-test
namespace: metallb-system
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;strong&gt;Client&lt;/strong&gt; - Fedora Core laptop i&amp;#8217;m writing this blog post on ;) I installed Bird and configured it to &lt;code&gt;import&lt;/code&gt; all bgp addresses from the &lt;code&gt;FRR&lt;/code&gt; neighbour as follows.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;dnf install -y bird

cat &amp;lt;&amp;lt;&apos;EOF&apos; &amp;gt; /etc/bird.conf
log syslog all;
protocol kernel {
        ipv4 {
              import none;
              export all;
        };
        ipv6 {
              import none;
              export all;
        };
}
protocol direct {
        disabled;               # Disable by default
        ipv4;                   # Connect to default IPv4 table
        ipv6;                   # ... and to default IPv6 table
}
protocol static {
        ipv4;
}
protocol device {
        scan time 10;
}
protocol bgp {
        description &quot;OpenShift FFR+MetalLB Routes&quot;;
        local as 64523;
        neighbor 192.168.86.23 as 64521;
        source address 192.168.86.109;
        ipv4 {
            import all;
            export none;
        };
}
EOF

systemctl start bird
journalctl -u bird.service&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_workload_demo&quot;&gt;Workload Demo&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OK, time to try this out with a real application on OpenShift. I am going to use a very simple hello world container.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Login to the SNO instance and create a namespace and a deployment.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc new-project welcome-metallb
oc create deployment welcome --image=quay.io/eformat/welcome:latest&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now create a &lt;code&gt;LoadBalancer&lt;/code&gt; type service, MetalLB will do its thing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f- &amp;lt;&amp;lt;&apos;EOF&apos;
---
apiVersion: v1
kind: Service
metadata:
  name: welcome
spec:
  selector:
    app: welcome
  ports:
    - port: 80
      protocol: TCP
      targetPort: 8080
  type: LoadBalancer
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can see an &lt;code&gt;ExternalIP&lt;/code&gt; was assigned along with a &lt;code&gt;NodePort&lt;/code&gt; by MetalLB.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc get svc

NAME      TYPE           CLUSTER-IP       EXTERNAL-IP       PORT(S)        AGE
welcome   LoadBalancer   172.30.154.119   192.168.155.150   80:30396/TCP   7s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If we describe the service, we can see that the address was also &lt;strong&gt;announced&lt;/strong&gt; over BGP.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc describe svc welcome

Name:                     welcome
Namespace:                welcome-metallb
Labels:                   &amp;lt;none&amp;gt;
Annotations:              &amp;lt;none&amp;gt;
Selector:                 app=welcome
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       172.30.154.119
IPs:                      172.30.154.119
LoadBalancer Ingress:     192.168.155.150
Port:                     &amp;lt;unset&amp;gt;  80/TCP
TargetPort:               8080/TCP
NodePort:                 &amp;lt;unset&amp;gt;  30396/TCP
Endpoints:                10.128.0.163:8080
Session Affinity:         None
External Traffic Policy:  Cluster
Events:
  Type    Reason        Age   From                Message
  ----    ------        ----  ----                -------
  Normal  IPAllocated   57s   metallb-controller  Assigned IP [&quot;192.168.155.150&quot;]
  Normal  nodeAssigned  57s   metallb-speaker     announcing from node &quot;sno&quot; with protocol &quot;bgp&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can check on our &lt;strong&gt;FRR&lt;/strong&gt; Host the BGP route was seen:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;[root@rhel8 ~]# podman exec -it frr-upstream   vtysh -c &quot;show ip route&quot;
Codes: K - kernel route, C - connected, S - static, R - RIP,
       O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,
       T - Table, v - VNC, V - VNC-Direct, A - Babel, F - PBR,
       f - OpenFabric,
       &amp;gt; - selected route, * - FIB route, q - queued, r - rejected, b - backup
       t - trapped, o - offload failure

K&amp;gt;* 0.0.0.0/0 [0/100] via 192.168.86.1, eth0, src 192.168.86.23, 19:16:24
C&amp;gt;* 192.168.86.0/24 is directly connected, eth0, 19:16:24
B&amp;gt;* 192.168.155.150/32 [20/0] via 192.168.86.32, eth0, weight 1, 00:02:12&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And from our &lt;strong&gt;Client&lt;/strong&gt; that Bird also added the route correctly from the announcement:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;route -n

Kernel IP routing table
Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
0.0.0.0         192.168.86.1    0.0.0.0         UG    600    0        0 wlp2s0
192.168.86.0    0.0.0.0         255.255.255.0   U     600    0        0 wlp2s0
192.168.155.150 192.168.86.23   255.255.255.255 UGH   32     0        0 wlp2s0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can try the app endpoint from our &lt;strong&gt;Client&lt;/strong&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ curl 192.168.155.150:80
Hello World ! Welcome to OpenShift from welcome-5575fd7854-7hlxj:10.128.0.163&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;🍾🍾 Yay ! success. 🍾🍾&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If we deploy the application &lt;em&gt;normally&lt;/em&gt; using a Route&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc new-project welcome-router
oc new-app quay.io/eformat/welcome:latest
oc expose svc welcome&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;and a &lt;code&gt;ClusterIP&lt;/code&gt; type &lt;code&gt;Service&lt;/code&gt;:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ oc get svc welcome
NAME      TYPE        CLUSTER-IP       EXTERNAL-IP   PORT(S)    AGE
welcome   ClusterIP   172.30.121.184   &amp;lt;none&amp;gt;        8080/TCP   62s&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We see that that MetalLB and normal HAProxy based Routing can happily co-exist in the same cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ curl welcome-welcome-router.apps.foo.eformat.me
Hello World ! Welcome to OpenShift from welcome-8dcc64fcd-2ktv4:10.128.0.167&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you delete the &lt;code&gt;welcome-metallb&lt;/code&gt; project or &lt;code&gt;LoadBalancer&lt;/code&gt; service, you will see the BGP announcement to remove the routing OK.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;🏅That&amp;#8217;s it !! Go forth and BGP !&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>Running Mastodon on OpenShift</title>
          <link>https://blog.eformat.me/2022/12/mastodon-openshift.html</link>
          <pubDate>Sat, 31 Dec 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/12/mastodon-openshift.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_join_the_fediverse_with_openshift&quot;&gt;Join the Fediverse with OpenShift&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Who knew that &lt;strong&gt;fediverse&lt;/strong&gt; was a portmanteau of &quot;federation&quot; and &quot;universe&quot; ? an ensemble of interconnected servers that are used for microblogging. If you are itching to try out your own Mastodon instance on OpenShift
i have just the &lt;a href=&quot;https://github.com/eformat/openshift-mastodon&quot;&gt;helm template&lt;/a&gt; for you.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It should be as simple as logging into OpenShift and running helm, where &lt;em&gt;CLUSTER_DOMAIN&lt;/em&gt; is your cluster apps domain name.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;helm upgrade --install my-fediverse . \
  --create-namespace --namespace mastodon \
  --set mastodon.local_domain=mastodon.&amp;lt;CLUSTER DOMAIN&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will get you a basic server installed, using the lastest Mastodon image. You should change the &lt;em&gt;values.yaml&lt;/em&gt; to adjust the default passwords and secrets prior to deploying anything other than a play-around instance - see the README.md for how to use rake to generate new secrets. Once deployed, you should see these pods running in your &lt;strong&gt;mastodon&lt;/strong&gt; namespace.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;mastodon-pods&quot;&gt;
  &lt;img src=&quot;/2022/12/mastodon-pods.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_a_note_on_s3&quot;&gt;A note on S3&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Mastodon can store its microblogging images in S3. The helm chart uses a &lt;a href=&quot;https://min.io&quot;&gt;minio&lt;/a&gt; instance running in OpenShift. In the default configuration, we want the s3 links to be publicly available via anonymous read-only access with the link, but not listable. For now we use the aws cli client to upload this policy manually post-install.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc -n mastodon port-forward svc/my-fediverse-minio 9000:9000
cat &amp;lt;&amp;lt; &apos;EOF&apos; &amp;gt; /tmp/mastodon-policy.json
{
  &quot;Version&quot;: &quot;2012-10-17&quot;,
  &quot;Statement&quot;: [
    {
      &quot;Action&quot;: [
        &quot;s3:GetObject&quot;
      ],
      &quot;Effect&quot;: &quot;Allow&quot;,
      &quot;Principal&quot;: {
        &quot;AWS&quot;: [
          &quot;*&quot;
        ]
      },
      &quot;Resource&quot;: [
        &quot;arn:aws:s3:::mastodon/*&quot;
      ],
      &quot;Sid&quot;: &quot;&quot;
    }
  ]
}
EOF

export AWS_PROFILE=minio
aws --endpoint-url http://localhost:9000 s3api put-bucket-policy --bucket mastodon --policy file:///tmp/mastodon-policy.json&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_logging_in_adding_users&quot;&gt;Logging In, Adding users&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By default users can self register to your mastodon instance. The user on boarding workflow uses email, so you can deploy using SMTP services. For example a popular service like &lt;a href=&quot;https://www.mailgun.com&quot;&gt;mailgun&lt;/a&gt; with your credentials would look something like this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;helm upgrade --install my-fediverse . \
  --set mastodon.smtp_server=smtp.mailgun.org \
  --set mastodon.smtp_login=postmaster@example.com \
  --set mastodon.smtp_password=123456 \
  --set mastodon.smtp_from_address=mastodon@example.com. \
  --create-namespace --namespace mastodon&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you do not want to set up SMTP just yet, we can also use a manual method. Browse to your mastodon front page and select &lt;strong&gt;Create Account&lt;/strong&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;mastodon-front-page&quot;&gt;
  &lt;img src=&quot;/2022/12/mastodon-front-page.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will let you sign up. We can rsh into the mastodon pod to manually approve the user. I signed up as &lt;strong&gt;eformat&lt;/strong&gt; and also gave myself the &lt;em&gt;Admin&lt;/em&gt; role.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc rsh $(oc get pods -l app.kubernetes.io/name=mastodon-streaming-mastodon -o name)

RAILS_ENV=production bin/tootctl accounts modify eformat --confirm
RAILS_ENV=production bin/tootctl accounts modify eformat --role Admin&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You should see &lt;strong&gt;OK&lt;/strong&gt; printed out when running these commands. Now log back in to mastodon and you should be able to right-click &lt;strong&gt;Preferences&lt;/strong&gt; to administer the server.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;mastodon-admin&quot;&gt;
  &lt;img src=&quot;/2022/12/mastodon-admin.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I updated the server thumbnail which is stored in your minio s3.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;mastodon-server-thumb&quot;&gt;
  &lt;img src=&quot;/2022/12/mastodon-server-thumb.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;🏅That&amp;#8217;s it !! you can find all of the &lt;a href=&quot;https://docs.joinmastodon.org/admin/config&quot;&gt;docs and configuration guides&lt;/a&gt; online for mastodon.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>Stable Diffusion on OpenShift with GPU Sharing</title>
          <link>https://blog.eformat.me/2022/12/nvidia-gpu-sharing.html</link>
          <pubDate>Tue, 13 Dec 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/12/nvidia-gpu-sharing.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_stable_diffusion_on_openshift_with_gpu_sharing&quot;&gt;Stable Diffusion on OpenShift with GPU Sharing&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So the intuitive follow on from the last blog post &lt;a href=&quot;https://blog.eformat.me/2022/11/stable-diffusion.html&quot;&gt;Stable Diffusion for Fedora Core&lt;/a&gt; is of course to see if we can get the app running on OpenShift in a lab environment!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are a couple of challenges. In my case, i actually wanted to demo the app in a lab that contains some older &lt;a href=&quot;https://www.nvidia.com/en-au/data-center/tesla-t4/&quot;&gt;Nvidia-Tesla-T4 GPU&amp;#8217;s&lt;/a&gt;, a bare metal SNO instance along with a bunch of other GPU enabled apps. This raises some interesting questions, in particular how do we configure and deploy applications so they can share the GPU&amp;#8217;s in this environment?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One of the best article i found &lt;a href=&quot;https://developer.nvidia.com/blog/improving-gpu-utilization-in-kubernetes&quot;&gt;describing GPU Sharing&lt;/a&gt; and the various mechanisms involved, highlights the different options available.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;gpu-concurrency-mechanisms&quot;&gt;
  &lt;img src=&quot;/2022/12/gpu-concurrency-mechanisms.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We are interested primarily in the system software and hardware part of this picture (CUDA and MPS-CUDA are more at the application level). Although, Stable Diffusion does require working CUDA for python torch as well.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;code&gt;MIG&lt;/code&gt; (which stands for multi instance GPU) is the newest technology and only supported on a small number of cards (not the T4&apos;) like vGPU (A100 and A30). There are some great &lt;a href=&quot;https://www.openshift.com/blog/multi-instance-gpu-support-with-the-gpu-operator-v1.7.0&quot;&gt;OpenShift blogs&lt;/a&gt; describing MIG usage. vGPU is a technology that is &lt;strong&gt;only&lt;/strong&gt; available if OpenShift is running in a VM/hypervisor. vGPUs are created/configured at the hypervisor level independently of OpenShift.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, that leaves us with &lt;strong&gt;Time-slicing&lt;/strong&gt;. The &lt;a href=&quot;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/time-slicing-gpus-in-openshift.html#configuring-gpus-with-time-slicing&quot;&gt;best place to read about it&lt;/a&gt; is on the Nvidia site. Unlike MIG, there is no memory or fault-isolation between replicas, but for some workloads this is better than not being able to share the GPU at all. &lt;a href=&quot;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/gpu-sharing.html&quot;&gt;There is a lot of documentation&lt;/a&gt; to read, so i&amp;#8217;m going to summarize the steps to get OpenShift Bare Metal SNO working using time-slicing.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_installing_the_node_feature_discovery_nfd_operator&quot;&gt;Installing the Node Feature Discovery (NFD) Operator&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The first step after installing OpenShift SNO bare-metal, was to configure the NFD operator as cluster-admin. The default configuration for the operator is fine. All going well, your GPU&amp;#8217;s should now be visible to OpenShift, and you can check by doing:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ oc debug node/&amp;lt;node name&amp;gt;
$ chroot /host
$ lspci | grep -i nvidia
17:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)
65:00.0 3D controller: NVIDIA Corporation TU104GL [Tesla T4] (rev a1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can see our two physical GPU&amp;#8217;s OK. Another check is the node labels and description:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ oc describe node | egrep &apos;Roles|pci&apos; | grep -v master
   feature.node.kubernetes.io/pci-10de.present=true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you see the &lt;strong&gt;pci-10de&lt;/strong&gt; device, that is the code for Nvidia GPU&amp;#8217;s, all good so far.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_installing_the_nvidia_gpu_operator&quot;&gt;Installing the NVIDIA GPU Operator&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next step is to &lt;a href=&quot;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/install-gpu-ocp.html&quot;&gt;install the Nvidia GPU Operator&lt;/a&gt;. By default you should &lt;strong&gt;not&lt;/strong&gt; need to install any license as &lt;a href=&quot;https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/openshift/steps-overview.html#entitlement-free-supported-versions&quot;&gt;OpenShift 4.9.9+ is entitlement free&lt;/a&gt;. There are several pods that install with this operator. If you install the default &lt;code&gt;Cluster Policy&lt;/code&gt; the nvidia driver is downloaded and compiled for your OpenShift and inserted as dynamic &lt;strong&gt;kmods&lt;/strong&gt;. This may take a little bit of time to complete.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;nvidia-driver&quot; class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/2022/12/nvidia-driver-pod.png&quot; alt=&quot;Nvidia Dameon Set&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In our case, we only have one node (SNO) so the dameon set compiles and installs the driver on our node. If you follow the documentation above you should be able to verify the drivers are loaded.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ oc debug node/&amp;lt;node name&amp;gt;
$ chroot /host
$ lsmod | grep nvidia
nvidia_modeset       1142784  0
nvidia_uvm           1310720  2
nvidia              40796160  363 nvidia_uvm,nvidia_modeset
drm                   589824  4 drm_kms_helper,nvidia,mgag200&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Its worth noting that if you were using vGPU, you would &lt;strong&gt;also&lt;/strong&gt; get the &lt;em&gt;nvidia_vgpu_vfio&lt;/em&gt; module, but because we are bare metal, the driver dameon set recognizes passthrough mode and does not compile it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The second part of the puzzle is you need to now configure the GPU for time-slicing. To do this we need create a ConfigMap that specifies how many slices we want, for example &lt;em&gt;8&lt;/em&gt; in our case.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;kind: ConfigMap
apiVersion: v1
metadata:
  name: time-slicing-config
  namespace: nvidia-gpu-operator
data:
  tesla-t4: |-
    version: v1
    sharing:
      timeSlicing:
        resources:
        - name: nvidia.com/gpu
          replicas: 8&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next, we add this ConfigMap name into the nvidia.com ClusterPolicy.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;                  devicePlugin:
                    config:
                      default: &quot;tesla-t4&quot;
                      name: &quot;time-slicing-config&quot;
                    enabled: true&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By enabling the &lt;em&gt;devicePlugin&lt;/em&gt; you should see the device plugin DaemonSet spin up.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;nvidia-deive-plugin&quot; class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/2022/12/nvidia-device-plugin.png&quot; alt=&quot;Nvidia Device Plugin Dameon Set&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We are nearly there ! If we now look at the OpenShift node description, we should see how many GPU&amp;#8217;s OpenShift now thinks it has.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ oc describe node| sed &apos;/Capacity/,/System/!d;/System/d&apos;

Capacity:
  ...
  nvidia.com/gpu:                 16
Allocatable:
  ...
  nvidia.com/gpu:                 16&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So great ! that is &lt;strong&gt;8x2=16&lt;/strong&gt; time-sliced GPU&amp;#8217;s available.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_deploy_stable_diffusion&quot;&gt;Deploy Stable Diffusion&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have created a simple &lt;a href=&quot;https://github.com/eformat/stable-diffusion/tree/main/openshift&quot;&gt;Kustomize folder&lt;/a&gt; in the git repo and split out the two part needed to get the app running.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;First create a data download job (this is 6 GB of downloads), which creates a PVC using he default Storage Class to download the required Stable Diffusion model data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f create-data/app.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Then run the deployment.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc apply -f create-app/app.yaml&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here&amp;#8217;s an example of a run on the lab, showing the &lt;code&gt;nvidia-smi pmon&lt;/code&gt; on the shell for the running python process and an output text to image.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;stable-diffusion-gpu-time-slice&quot;&gt;
  &lt;img src=&quot;/2022/12/stable-diffusion-gpu-time-slice.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;In our Deployment we only requested one GPU, so we get one time-sliced gpu.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;        resources:
          limits:
            nvidia.com/gpu: 1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can scale this up, or use the nvidia sample image to test out time-slicing and sharing e.g. Create a Deployment using this image.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;        replicas: 16
        image: nvidia/samples:dcgmproftester-2.0.10-cuda11.0-ubuntu18.04
        resources:
          limits:
            nvidia.com/gpu: &quot;1&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;And hey presto ! we now see 15/16 app replicas spinning up and running on our 2 physical GPU&amp;#8217;s. You can see them easily using &lt;code&gt;nvidia-smi pmon&lt;/code&gt;. We don&amp;#8217;t quite get to 16 as Stable Diffusion is still running on the GPU as well!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;stable-diffusion-gpu-time-slice&quot;&gt;
  &lt;img src=&quot;/2022/12/gpu-sharing-16.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>Stable Diffusion for Fedora Core</title>
          <link>https://blog.eformat.me/2022/11/stable-diffusion.html</link>
          <pubDate>Wed, 23 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/stable-diffusion.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_stable_diffusion_for_fedora_core&quot;&gt;Stable Diffusion for Fedora Core&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If you have not heard about it, &lt;code&gt;Stable Diffusion&lt;/code&gt; &lt;a href=&quot;https://stability.ai/blog/stable-diffusion-public-release&quot;&gt;is a text to image ML model generator&lt;/a&gt;. I wanted to demo a GPU with podman and OCI like a pro and I don&amp;#8217;t want to use to the awesome but boring &lt;code&gt;docker.io/nvidia/samples:vectoradd-cuda11.2.1&lt;/code&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Watching numbers add .. erm, yeah. This is 2022 baby!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So let&amp;#8217;s see if we can build this &lt;code&gt;Stable Diffusion&lt;/code&gt; thing on fedora. The setup is &lt;em&gt;painfull&lt;/em&gt; .. I warn you now. But &lt;em&gt;its worth the effort&lt;/em&gt; - trust me&amp;#8230;&amp;#8203;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_getting_setup&quot;&gt;Getting Setup&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have a 4 (nearly 5 ?) year old work laptop - a &lt;code&gt;dell-xps-15&lt;/code&gt;. It has been a real workhorse 🐴 ! But, if you have been running fedora for as long as i have, you will know that running NVIDIA graphics has been .. well, torturous to say the least over the years. Things have gotten &lt;em&gt;way better&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;linus-nvidia&quot; class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/2022/11/linus-nvidia.jpg&quot; alt=&quot;Linus &quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So strap yourself in ! At least these days, NVIDIA play very nicely in the Open Source community, so hopefully you will not need to &lt;em&gt;&quot;do a Linus&quot;&lt;/em&gt; as we probably all have over the years.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Here&amp;#8217;s my hardware devices:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ lspci | egrep -i &apos;vga|nvidia&apos;
00:02.0 VGA compatible controller: Intel Corporation HD Graphics 630 (rev 04)
01:00.0 3D controller: NVIDIA Corporation GP107M [GeForce GTX 1050 Mobile] (rev a1)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Its worth noting that i run my display with the perfectly acceptable &lt;strong&gt;intel i915 gpu&lt;/strong&gt; (on the mother board). For one thing, it runs a lot cooler than the NVIDIA card, so less noisy overall as the fans don&amp;#8217;t scream. You can blacklist the nvidia drivers then &lt;code&gt;dracut -f&lt;/code&gt; the boot image:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ cat /etc/modprobe.d/blacklist_video.conf
blacklist nouveau
blacklist lbm-nouveau
blacklist nvidia-current
alias nvidia nvidia_drm nvidia_modeset nvidia_current_updates
alias nouveau off
alias lbm-nouveau off&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OR, you can set up the kernel to boot using &lt;code&gt;i915.modeset&lt;/code&gt; and blacklist there as well. I also blacklist the default &lt;code&gt;nouveau&lt;/code&gt; driver because, err - it runs like a dog! You never want to use it when you have other options, like two perfectly good graphics card drivers to choose from!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ cat /etc/default/grub
GRUB_CMDLINE_LINUX=&quot;i915.modeset=1 quiet rhgb intel_iommu=on modprobe.blacklist=nouveau,nvidia,nvidia_drm,nvidia_modeset rd.driver.blacklist=nouveau&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One other thing i find very handy, is to run graphics using the intel gpu, and then use the NVIDIA gpu for AIML, or to pass through via PCI to OpenShift SNO running in &lt;code&gt;libvirt&lt;/code&gt;, so i set &lt;code&gt;intel_iommu=on&lt;/code&gt; as well for good measure. This seems to confuse people, but if you want to pci passthrough the device to libvirt &amp;#8230;&amp;#8203; you &lt;strong&gt;cannot&lt;/strong&gt; share it ! i.e. don&amp;#8217;t run your main monitor using the NVIDIA card, and expect to share it with a VM using pci-passthrough.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Make sure to recreate your (in my case UEFI) bootloader &lt;code&gt;grub2-mkconfig -o /boot/efi/EFI/fedora/grub.cfg&lt;/code&gt; if you change any of these.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Sadly, you are not done. What you need now is a running NVIDIA, CUDA drivers for your operating system. I&amp;#8217;m running fc36. So many years, so much pain here, so many crappy blogs giving you bad advice. OK .. so this is my advice, if you are starting with a broken system, &lt;code&gt;dnf erase nvidia*&lt;/code&gt; is your best bet. Start from a clean state.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For fc36, use the f35 repo. There is also a f36 repo &lt;a href=&quot;https://forums.developer.nvidia.com/t/bug-report-on-nvidia-driver-515-65-01-for-fedora-36-kernel-5-18-19-rtx-2060-rev-1/227009/7&quot;&gt;that is known not to work!&lt;/a&gt;. Why is this? i don&amp;#8217;t know, i have not debugged the C/C++ yet, but &lt;code&gt;dkms&lt;/code&gt; will fail to compile the kernel driver if you try the fc36 repo (and the nvidia driver version is lower, so go figure?).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;dnf config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/fedora35/x86_64/cuda-fedora35.repo
dnf -y module install nvidia-driver:latest-dkms
dnf -y install cuda&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Now, you will also need (cuDNN) which is a GPU-accelerated library of primitives for deep neural networks. The easiest way i found to install this is to grab the local repo as rpm and install it. You &lt;a href=&quot;https://developer.nvidia.com/rdp/cudnn-archive&quot;&gt;need to download it here from nvidia&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;dnf -y install cudnn-local-repo-rhel8-8.5.0.96-1.0-1
tree /var/cudnn-local-repo-rhel8-8.5.0.96/
dnf install -y /var/cudnn-local-repo-rhel8-8.5.0.96/libcudnn8-8.5.0.96-1.cuda11.7.x86_64.rpm
dnf erase cudnn-local-repo-rhel8-8.5.0.96-1.0-1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once you have &lt;code&gt;libcudnn&lt;/code&gt; installed, you can uninstall the local repo. There may be a better way, but 🤷&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I use a simple shell script to load my nvidia driver when i need it.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ cat ~/bin/nvidia.sh
#!/bin/bash
# we have these blacklisted on boot so we can load i915
sudo dkms status
sudo modprobe nvidia_drm modeset=1 nvidia_modeset nvidia
sudo ldconfig&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For &lt;code&gt;podman&lt;/code&gt;, you will need to do the following&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;install nvidia drivers as above, make sure &lt;code&gt;nvidia-smi&lt;/code&gt; works on the host (see testing in next section)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Install &lt;code&gt;nvidia-container-toolkit&lt;/code&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;curl -s -L https://nvidia.github.io/libnvidia-container/rhel8.6/libnvidia-container.repo | sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo
dnf install -y nvidia-container-toolkit&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Modify &lt;code&gt;/etc/nvidia-container-runtime/config.toml&lt;/code&gt; and change these values (needed because of &lt;code&gt;cgroupsv2&lt;/code&gt; and the desire to run the pod rootless if possible)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;[nvidia-container-cli]
#no-cgroups = false
no-cgroups = true
#user = root:video
user = &quot;root:root&quot;
[nvidia-container-runtime]
#debug = &quot;/var/log/nvidia-container-runtime.log&quot;
debug = &quot;~/./local/nvidia-container-runtime.log&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You should now be good to go.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_test_your_local_setup&quot;&gt;Test Your Local Setup&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s get some things working. Needless to say if any of these steps fail, you are going to have to debug and fix them 🛠️ !&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This is always my first check, from your shell:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ nvidia-smi

Wed Nov 23 05:21:19 2022
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 520.61.05    Driver Version: 520.61.05    CUDA Version: 11.8     |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|                               |                      |               MIG M. |
|===============================+======================+======================|
|   0  NVIDIA GeForce ...  Off  | 00000000:01:00.0 Off |                  N/A |
| N/A   56C    P8    N/A /  N/A |      0MiB /  4096MiB |      0%      Default |
|                               |                      |                  N/A |
+-------------------------------+----------------------+----------------------+

+-----------------------------------------------------------------------------+
| Processes:                                                                  |
|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |
|        ID   ID                                                   Usage      |
|=============================================================================|
|  No running processes found                                                 |
+-----------------------------------------------------------------------------+&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If podman setup correctly this will also work in a pod (note this is rootless and done as my normal user):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;podman run --rm --security-opt=label=disable \
--hooks-dir=/usr/share/containers/oci/hooks.d/ \
docker.io/nvidia/cuda:11.2.2-base-ubi8 \
/usr/bin/nvidia-smi&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can now check that a python container will find your GPU and CUDA setup correctly. Stable Diffusion uses the &lt;a href=&quot;http://torch.ch/&quot;&gt;torch&lt;/a&gt; library, but if things don&amp;#8217;t work tensorflow gives you a lot more details about any failure (libraries, cuda version mismatch etc). It is worth pointing out you &lt;strong&gt;must&lt;/strong&gt; have the same CUDA libs in both places (your host and image), so make sure you &lt;strong&gt;do&lt;/strong&gt;! (see the Dockerfile for Stable Diffusion later on).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;podman run --rm -it --privileged \
--security-opt=label=disable \
--hooks-dir=/usr/share/containers/oci/hooks.d/ \
docker.io/tensorflow/tensorflow:latest-gpu&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You should be able to check that the &lt;code&gt;nvidia&lt;/code&gt; device is available in the pod:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;root@3e8c8ba4e6fb:/# ls -lart /dev/nvidia0
crw-rw-rw-. 1 nobody nogroup 195, 0 Nov 23 01:26 /dev/nvidia0&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Then check that tensorflow can see your GPU, this will give you detailed messages if it cannot find your drivers and libraries:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;root@3e8c8ba4e6fb:/# python3.8
Python 3.8.10 (default, Jun 22 2022, 20:18:18)
[GCC 9.4.0] on linux
Type &quot;help&quot;, &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.
&amp;gt;&amp;gt;&amp;gt; import tensorflow as tf
2022-11-23 06:37:46.901772: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
...
&amp;gt;&amp;gt;&amp;gt; tf.test.gpu_device_name()
2022-11-23 06:37:52.706585: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /device:GPU:0 with 3364 MB memory:  -&amp;gt; device: 0, name: NVIDIA GeForce GTX 1050, pci bus id: 0000:01:00.0, compute capability: 6.1
&apos;/device:GPU:0&apos;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The last line &lt;code&gt;&apos;/device:GPU:0&apos;&lt;/code&gt; is good. Now, we can also check torch works (you can leave this check till later, once you have built or pulled the &lt;code&gt;Stable Diffusion&lt;/code&gt; image)&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ pip3.8 install torch --user
$ python3.8 -c &quot;import torch; print(torch.cuda.is_available())&quot;
True&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;If that returns False, then something is amiss.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_build_the_stable_diffusion_image&quot;&gt;Build the Stable Diffusion Image&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OK, the instructions from here should be straight forward and i have &lt;a href=&quot;https://github.com/eformat/stable-diffusion/&quot;&gt;put the instructions in a git repo here&lt;/a&gt;. Strictly speaking you can just grab the image and run it if you have a similar setup to mine &lt;code&gt;podman pull quay.io/eformat/sd-auto:14-02&lt;/code&gt;. Be warned its a 6GB image!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You will need some time ☕ and storage available! The AI model downloads use approx (12GB) of local disk 😲 and we use the &lt;code&gt;aria2&lt;/code&gt; torrent client to grab all the bits needed.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Download the data.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;dnf -q install aria2
./download.sh&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Then we can build the container using podman. There is a &lt;code&gt;Makefile&lt;/code&gt; to make your life easier.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;make build&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Then we can run it using podman. Note: you have to mount the &lt;code&gt;download/data&lt;/code&gt; folder so set &lt;code&gt;DATA_DIR=&amp;lt;full path&amp;gt;/download/data&lt;/code&gt; appropriately. We also run the pod as privileged which should not ne strictly be necessary (/dev/nvidia0 is not found otherwise, this needs fixing up).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;podman run --privileged -it -p 7860:7860 -e CLI_ARGS=&quot;--allow-code --medvram --xformers&quot; \
-v $DATA_DIR:/data:Z \
--security-opt=label=disable \
--hooks-dir=/usr/share/containers/oci/hooks.d/ \
quay.io/eformat/sd-auto:14-02&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_demo_it&quot;&gt;Demo It!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Browse to &lt;code&gt;&lt;a href=&quot;http://0.0.0.0:7860/&quot; class=&quot;bare&quot;&gt;http://0.0.0.0:7860/&lt;/a&gt;&lt;/code&gt; and type in some text. In this example i was using:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;text&quot;&gt;forest wanderer by dominic mayer, anthony jones, Loish, painterly style by Gerald parel, craig mullins, marc simonetti, mike mignola, flat colors illustration, bright and colorful, high contrast, Mythology, cinematic, detailed, atmospheric, epic , concept art, Matte painting, Lord of the rings, Game of Thrones, shafts of lighting, mist, , photorealistic, concept art, volumetric light, cinematic epic + rule of thirds&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You should get an awesome image generated!&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/2022/11/tmpcgvezq90.png&quot; alt=&quot;Image &quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can also check the python process is running using your GPU OK by running:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;text&quot;&gt;$ nvidia-smi pmon&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Which you can see with the application and shell side by side here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;ddd-school-timetable-classes&quot;&gt;
  &lt;img src=&quot;/2022/11/stable-diffusion.png&quot; class=&quot;zoom&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;🎉🎉 Enjoy 🎉🎉&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_attribution&quot;&gt;Attribution&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Code was forked with 💕 from here. Check it out if you want to build other UI&amp;#8217;s to demo with.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/AbdBarho/stable-diffusion-webui-docker&quot; class=&quot;bare&quot;&gt;https://github.com/AbdBarho/stable-diffusion-webui-docker&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>SNO in AWS for $150/mo</title>
          <link>https://blog.eformat.me/2022/11/aws-sno-150.html</link>
          <pubDate>Thu, 10 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/aws-sno-150.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_so_you_want_to_demo_openshift_like_a_boss&quot;&gt;So you want to demo OpenShift like a boss &amp;#8230;&amp;#8203;&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div id=&quot;money&quot; class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;span class=&quot;image&quot;&gt;&lt;img src=&quot;/2022/11/100-unsplash.jpg&quot; alt=&quot;Money&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;&lt;/span&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;What is the cheapest way to run OpenShift in the public cloud ?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Behold .. the awesomeness-ness of SNO (Single Node OpenShift) on persistent &lt;a href=&quot;https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-spot-instances.html&quot;&gt;spot in AWS&lt;/a&gt;. A Spot Instance is an instance that uses spare EC2 capacity that is available for a lot less than the On-Demand price. How much less ? well.. you can &lt;a href=&quot;https://aws.amazon.com/ec2/spot/pricing&quot;&gt;check it out here&lt;/a&gt; but normally 70% less ec2 cost. Just get used to some interruptions 😶‍🌫️.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For installing and demoing &lt;em&gt;anything&lt;/em&gt; in OpenShift you will normally need a bare minimum of 8vCPU and 32 GB RAM for SNO which may get you close to under the $100 mark 😲.&lt;/p&gt;
&lt;/div&gt;
&lt;table class=&quot;tableblock frame-ends grid-all&quot; style=&quot;width: 50%;&quot;&gt;
&lt;colgroup&gt;
&lt;col style=&quot;width: 33.3333%;&quot;&gt;
&lt;col style=&quot;width: 33.3333%;&quot;&gt;
&lt;col style=&quot;width: 33.3334%;&quot;&gt;
&lt;/colgroup&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;/th&gt;
&lt;th class=&quot;tableblock halign-center valign-top&quot; colspan=&quot;2&quot;&gt;Price&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;m6a.2xlarge&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-center valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;$0.1658 per Hour&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;em&gt;$120&lt;/em&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;GP3 volumes&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-center valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;approx&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;em&gt;$10&lt;/em&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;ELB+EIP&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-center valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;code&gt;approx&lt;/code&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;em&gt;$20&lt;/em&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;tfoot&gt;
&lt;tr&gt;
&lt;td class=&quot;tableblock halign-right valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;strong&gt;Total:&lt;/strong&gt;&lt;/p&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-center valign-top&quot;&gt;&lt;/td&gt;
&lt;td class=&quot;tableblock halign-left valign-top&quot;&gt;&lt;p class=&quot;tableblock&quot;&gt;&lt;em&gt;$150&lt;/em&gt;&lt;/p&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;/tfoot&gt;
&lt;/table&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;But others could suit your need better:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;c5n.4xlarge - 16 vCPU, 42 GB RAM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;m6a.2xlarge - 8 vCPU, 32 GB RAM&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;r6i.2xlarge - 8 vCPU, 64 GB RAM&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Prices will vary over time ! it is spot after all. The rate of instance &lt;a href=&quot;https://aws.amazon.com/ec2/spot/instance-advisor&quot;&gt;interruption&lt;/a&gt; also varies by region and instance type, so I pick and choose based on latency to where I work from.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, how do we get there ?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;💥 &lt;strong&gt;UPDATE&lt;/strong&gt; - Checkout the automation here - &lt;a href=&quot;https://github.com/eformat/sno-for-100&quot; class=&quot;bare&quot;&gt;https://github.com/eformat/sno-for-100&lt;/a&gt; 💥&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_configuring_and_installing_openshift&quot;&gt;Configuring and Installing OpenShift&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can &lt;a href=&quot;https://docs.openshift.com/container-platform/4.11/installing/installing_sno/install-sno-installing-sno.html&quot;&gt;check the docs&lt;/a&gt; for configuring the install.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ openshift-install create install-config&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You want to install SNO, so your config should look similar to this:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;apiVersion: v1
baseDomain: &amp;lt;your base domain&amp;gt;
compute:
- name: worker
  replicas: 0
controlPlane:
  name: master
  replicas: 1
  architecture: amd64
  hyperthreading: Enabled
  platform:
    aws:
      type: c5n.4xlarge
      rootVolume:
        size: 250
        type: gp3
metadata:
  name: sno
platform:
  aws:
    region: &amp;lt;your region&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You want a single master, choose how big you want your root volume and instance size and which region to install to. Personally I use &lt;a href=&quot;https://github.com/openshift/hive/blob/master/docs/clusterpools.md&quot;&gt;Hive and ClusterPools&lt;/a&gt; from an SNO instance in my home lab to install all my public cloud clusters, that way I can easily control then via configuration and &lt;a href=&quot;https://github.com/openshift/hive/blob/master/docs/hibernating-clusters.md&quot;&gt;hibernate&lt;/a&gt; them when I want ! You can also just install via the cli of course:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ openshift-install create cluster&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_adjusting_sno_to_remove_all_the_costly_networking_bits&quot;&gt;Adjusting SNO to remove all the costly networking bits!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When you install SNO, it installs a bunch of stuff you may not want in a demo/lab environment. With a single node, the load balancers and the private routing are usually not necessary at all. It&amp;#8217;s always possible to put the private routing and subnets back if you need to add workers later or just reinstall.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I am going to include the aws cli commands as guidance, they need a bit more polish to make them fully scriptable, but we&amp;#8217;re working on it ! This saves you approx~ $120/mo for the 3 NAT gateways, $40/mo for 2 API load balancers and $10/mo for 2 EIP&amp;#8217;s. I will keep the router ELB.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Update Master Security Group: Allow 6443 (TCP)&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;region=&amp;lt;your aws region&amp;gt;
instance_id=&amp;lt;your instance id&amp;gt;
master_sg_name=&amp;lt;your cluster&amp;gt;-sno-master-sg

sg_master=$(aws ec2 describe-security-groups \
  --region=${region} \
  --query &quot;SecurityGroups[].GroupId&quot; \
  --filters &quot;Name=vpc-id,Values=${vpc}&quot; \
  --filters &quot;Name=tag-value,Values=${master_sg_name}&quot; | jq -r .[0])

aws ec2 authorize-security-group-ingress \
--region=${region} \
--group-id ${sg_master} \
--ip-permissions &apos;[{&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: 6443, &quot;ToPort&quot;:6443, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]}]&apos;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update Master Security Group: Allow 30000 to 32767 (TCP &amp;amp; UDP) from 0.0.0.0/0 for NodePort services&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws ec2 authorize-security-group-ingress \
--region=${region} \
--group-id ${sg_master} \
--ip-permissions &apos;[{&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: 30000, &quot;ToPort&quot;:32767, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]},{&quot;IpProtocol&quot;: &quot;udp&quot;, &quot;FromPort&quot;: 30000, &quot;ToPort&quot;:32767, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]}]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Add Security Groups that were attached to Routing ELB to master&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws ec2 authorize-security-group-ingress \
--region=${region} \
--group-id ${sg_master} \
--ip-permissions &apos;[{&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: 443, &quot;ToPort&quot;:443, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]},{&quot;IpProtocol&quot;: &quot;tcp&quot;, &quot;FromPort&quot;: 80, &quot;ToPort&quot;:80, &quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]},{&quot;IpProtocol&quot;: &quot;icmp&quot;, &quot;FromPort&quot;: 8, &quot;ToPort&quot;: -1,&quot;IpRanges&quot;: [{&quot;CidrIp&quot;: &quot;0.0.0.0/0&quot;}]}]&apos;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Attach a new public elastic IP address&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;eip=$(aws ec2 allocate-address --domain vpc --region=${region})

aws ec2 associate-address \
--region=${region} \
--allocation-id $(echo ${eip} | jq -r &apos;.AllocationId&apos;) \
--instance-id ${instance_id}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Update all subnets to route through IGW (using public route table)&lt;/p&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;# update public route table and add private subnets to route through igw (using public route table), public subnets already route that way
aws ec2 describe-route-tables --filters &quot;Name=vpc-id,Values=${vpc}&quot; --region=${region} &amp;gt; /tmp/baz

# inspect /tmp/baz to get the right id&apos;s, update them individually
aws ec2 replace-route-table-association \
--association-id rtbassoc-&amp;lt;id&amp;gt; \
--route-table-id rtb-&amp;lt;id for igw&amp;gt; \
--region=${region}&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Route53: Change API, APPS - A record to elastic IP address&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Route53: Change internal API, APPS - A records to private IP address of instance&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I&amp;#8217;m just going to list the generic command here, rinse and repeat for each of the zone records (four times, [int, ext] - for [*.apps and api]):&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws route53 list-hosted-zones

# get your hosted zone id&apos;s
hosted_zone=/hostedzone/&amp;lt;zone id&amp;gt;

# use the private ip address for the internal zone
cat &amp;lt;&amp;lt; EOF &amp;gt; /tmp/route53_policy1
{
            &quot;Changes&quot;: [
              {
                &quot;Action&quot;: &quot;UPSERT&quot;,
                &quot;ResourceRecordSet&quot;: {
                  &quot;Name&quot;: &quot;api.&amp;lt;your cluster domain&amp;gt;&quot;,
                  &quot;Type&quot;: &quot;A&quot;,
                  &quot;TTL&quot;: 300,
                  &quot;ResourceRecords&quot;: [
                    {
                      &quot;Value&quot;: &quot;$(echo $eip | jq -r &apos;.PublicIp&apos;)&quot;
                    }
                  ]
                }
              }
            ]
          }
EOF

aws route53 change-resource-record-sets \
--region=${region} \
--hosted-zone-id $(echo ${hosted_zone} | sed &apos;s/\/hostedzone\///g&apos;) \
--change-batch file:///tmp/route53_policy1&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete NAT gateways&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will delete all your nat gateways, adjust to suit&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;for i in `aws ec2 describe-nat-gateways --region=${region} --query=&quot;NatGateways[].NatGatewayId&quot; --output text | tr &apos;\n&apos; &apos; &apos;`; do aws ec2 delete-nat-gateway --nat-gateway-id ${i} --region=${region}; done&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Release public IP addresses (from NAT gateways)&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There will be two public EIP&amp;#8217;s you can now release:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws ec2 release-address \
--region=${region} \
--public-ip &amp;lt;public ip address&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete API load balancers (ext, int)&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will delete all your api load balancers, adjust to suit&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;for i in `aws elb describe-load-balancers --region=${region} --query=&quot;LoadBalancerDescriptions[].LoadBalancerName&quot; --output text | tr &apos;\n&apos; &apos; &apos;`; do aws elb delete-load-balancer --region=${region} --load-balancer-name ${i}; done&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Delete API load balancer target groups&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;FIXME - need to look these up&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws elbv2 delete-target-group \
--target-group-arn arn:aws:elasticloadbalancing:us-west-2:123456789012:targetgroup/my-targets/73e2d6bc24d8a067&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Use Host Network for ingress&lt;/p&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;FIXME - extra step&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc -n openshift-ingress-operator patch ingresscontrollers/default --type=merge --patch=&apos;{&quot;spec&quot;:{&quot;endpointPublishingStrategy&quot;:{&quot;type&quot;:&quot;HostNetwork&quot;,&quot;hostNetwork&quot;:{&quot;httpPort&quot;: 80, &quot;httpsPort&quot;: 443, &quot;protocol&quot;: &quot;TCP&quot;, &quot;statsPort&quot;: 1936}}}}&apos;
oc -n openshift-ingress delete services/router-default&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Restart SNO to ensure it still works !&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_convert_sno_to_spot&quot;&gt;Convert SNO to SPOT&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This has the effect of creating a spot request which will be permanent and only stop the instance should the price or capacity not be met temporarily. We&amp;#8217;re using &lt;a href=&quot;https://pythonawesome.com/a-tool-to-convert-aws-ec2-instances-back-and-forth-between-on-demand&quot;&gt;this script&lt;/a&gt; to convert the SNO instance:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;$ ./ec2-spot-converter --stop-instance --review-conversion-result --instance-id &amp;lt;your instance id&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This will take a bit of time to run and gives good debugging info. You can delete any temporary ami&amp;#8217;s and snapshots it creates.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_a_little_work_in_progress&quot;&gt;A little work in progress &amp;#8230;&amp;#8203;&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The conversion script changes your instance id to a new one during the conversion. This stops the instance from registering in the router ELB properly. So we need to update the instance id in a few places in SNO - for now we need to do the following steps.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Update the machine api object&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc edit machine -n openshift-machine-api
# change your .spec.providerID to &amp;lt;your new converted instance id&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;To make this survive a restart, we need to change the aws service provider id by hand on disk.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc debug node/&amp;lt;your node name&amp;gt;
chroot /host
cat /etc/systemd/system/kubelet.service.d/20-aws-providerid.conf

# the file will look like this with your region and instance
[Service]
Environment=&quot;KUBELET_PROVIDERID=aws:///&amp;lt;region&amp;gt;/&amp;lt;your original instance id&amp;gt;&quot;

# edit this file using vi and change &amp;lt;your original instance id&amp;gt; -&amp;gt; &amp;lt;your new converted instance id&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Delete the node ! the kubelet will re-register itself on reboot # restart the service&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;oc delete node &amp;lt;your node name&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Restart SNO&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You can check the instance is correctly registered to the ELB.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;aws elb describe-load-balancers \
--region=${region} \
--query=&quot;LoadBalancerDescriptions[].Instances&quot; \
--output text&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I will update this blog if we get a better way to manage this instance id thing over time 🤞🤞🤞&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_profit&quot;&gt;Profit !&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;💸💸💸 You should now be off to the races 🏇🏻 with your cheap-as SNO running on Spot.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The next steps - normally I would add a Lets Encrypt Cert, add users and configure the LVM Operator for thin-lvm based storage class. That i will leave those steps for another blog. Enjoy. 🤑&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;sre-cluster-argo-team-namespaced&quot;&gt;
  &lt;img src=&quot;/2022/11/sno-aws.png&quot; class=&quot;zoom&quot;&gt;
  &lt;div class=&quot;title&quot;&gt;SNO for $150/mo in AWS on c5n.4xlarge&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>AI Constraints Programming with Quarkus and OptaPlanner</title>
          <link>https://blog.eformat.me/2022/11/optaplanner-quarkus.html</link>
          <pubDate>Fri, 4 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/optaplanner-quarkus.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_ai_on_quarkus_i_love_it_when_an_optaplan_comes_together&quot;&gt;AI on Quarkus: I love it when an OptaPlan comes together&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have been meaning to look at &lt;a href=&quot;https://www.optaplanner.org/&quot;&gt;OptaPlanner&lt;/a&gt; for ages. All i can say is &lt;em&gt;&quot;Sorry Geoffrey De Smet, you are a goddamn genius and i should have played with OptaPlanner way sooner&quot;.&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So, i watched this video to see how to get started.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;videoblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;iframe src=&quot;https://www.youtube.com/embed/3N3NoDs3Ylc?rel=0&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So much fun ! 😁 to code.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;quarkus-school-timetabling-screenshot&quot;&gt;
  &lt;img src=&quot;/2022/11/quarkus-school-timetabling-screenshot.png&quot; class=&quot;zoom&quot;&gt;
  &lt;div class=&quot;title&quot;&gt;Figure - Quarkus School Timetable&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There were a couple of long learnt lessons i remembered whilst playing with the code.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;(1) Domain Driven Design&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To get at the heart of constraints programming you need a good object class hierarchy, one that is driven by your business domain. Thanks Eric Evans for the gift that keeps giving - DDD (and UML) is perfect to help you out here.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;ddd-school-timetable-classes&quot;&gt;
  &lt;img src=&quot;/2022/11/ddd-school-timetable-classes.png&quot; class=&quot;zoom&quot;&gt;
  &lt;div class=&quot;title&quot;&gt;Figure - Class Hierarchy&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;You need to have a clean and well thought out class heirarchy so that wiring in OptaPlanner will work for you. I can see several iterations and workshop sessions ensuing to get to a workable and correct understanding of the problem domain.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;(2) Constraints Programming&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I went looking for some code i helped write some 15 years ago ! A &lt;a href=&quot;https://en.wikipedia.org/wiki/Constraint_programming&quot;&gt;Constraint&lt;/a&gt; based programming model we had written in C++&lt;/p&gt;
&lt;/div&gt;
&lt;script src=&quot;https://gist.github.com/eformat/87efba0e2ec717ff077852c5924766ec.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We had a whole bunch of Production classes used for calculating different trades types and their values. You added these &lt;em&gt;productions&lt;/em&gt; into a solver class heirarcy and if you had the right &lt;a href=&quot;https://en.wikipedia.org/wiki/Degrees_of_freedom&quot;&gt;degrees of freedom&lt;/a&gt; your trade calculation would be successful. The beauty of it was the solver would spit out any parameter you had not specified, as long as it was possible to calculate it based on the production rules.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OptaPlanner viscerally reminded me of that code and experience, and started me thinking about how to use it for a similar use case.&lt;/p&gt;
&lt;/div&gt;
&lt;script src=&quot;https://gist.github.com/eformat/6551fe9434bb0a810321c83bd07adee2.js&quot;&gt;&lt;/script&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I am now a fan 🥰&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One last lesson from the OptaPlanner crew was their use of a a new static doc-generation system, their docs are a thing of beauty i have to say, &lt;a href=&quot;https://jbake.org&quot;&gt;JBake&lt;/a&gt; which I am using to write this blog with. Thanks for all the fish 🐟 🐠 Geoff.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OptaPlanner Quickstarts Code - &lt;a href=&quot;https://github.com/kiegroup/optaplanner-quickstarts&quot; class=&quot;bare&quot;&gt;https://github.com/kiegroup/optaplanner-quickstarts&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>Patterns with ArgoCD - Vault</title>
          <link>https://blog.eformat.me/2022/11/argocd-patterns-vault.html</link>
          <pubDate>Fri, 4 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/argocd-patterns-vault.html</guid>
          <description>
              &lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_team_collaboration_with_argocd&quot;&gt;Team Collaboration with ArgoCD&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I have written before about collaborating using GitOps, ArgoCD and Red Hat GitOps Operator. How can we better align our deployments with our teams ?&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The array of patterns and the helm chart are described &lt;a href=&quot;https://github.com/redhat-cop/helm-charts/blob/master/charts/gitops-operator/TEAM_DOCS.md&quot;&gt;in a fair bit of detail here&lt;/a&gt;. I want to talk about using one of these patterns at scale - hundred&amp;#8217;s of apps across multiple clusters.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_platform_cluster_argocd_tenant_team_argocds&quot;&gt;Platform Cluster ArgoCD, Tenant Team ArgoCD&amp;#8217;s&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For Product Teams working in large organisations that have a central Platform Team - this pattern is probably the most natural i think.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;div class=&quot;imageblock id=&quot;sre-cluster-argo-team-namespaced&quot;&gt;
  &lt;img src=&quot;/2022/11/sre-cluster-argo-team-namespaced.png&quot; class=&quot;zoom&quot;&gt;
  &lt;div class=&quot;title&quot;&gt;Figure - Platform ArgoCD, Namespaced ArgoCD per Team&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It allows the platform team to control cluster and elevated privileges, activities like controlling namespaces, configuring cluster resources etc, in their Cluster Scoped ArgoCD, whilst Product Teams can control their namespaces independently of them.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;The RedHat GitOps Operator (cluster scoped)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Platform Team (cluster scoped) ArgoCD instance&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Team (namespace scoped) ArgoCD instances&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;When doing multi-cluster, i usually prefer to have ArgoCD &lt;em&gt;&quot;in the cluster&quot;&lt;/em&gt; rather than remotely controlling a cluster. This seems better from an availability / single point of failure point of view. Of course if its a more &lt;em&gt;edge&lt;/em&gt; use case, remote cluster connections may make sense.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OK, so the Tenant ArgoCD is deployed in &lt;strong&gt;namespaced&lt;/strong&gt; mode and controls multiple namespaces belonging to a team. For each Team, a single ArgoCD instance per cluster normally suffices. You can scale up and shard the argo controllers, run in HA - not usually necessary at team scale (100 apps) - see the argocd doco if you need to do this though. There may be multiple non-production clusters - dev, test, qa etc and then you will have multiple production clusters (prod + dr etc) - each cluster have their own ArgoCD instances per Team.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;All of this is controlled via gitops. A sensible code split is one git repo per team, so one for the Platform Team, one for each Product Team - i normally start with a mono repo and split later based on need or scale.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It is worth pointing out that any elevated cluster RBAC permissions that are needed by the Product Teams&apos; are done via git PR&amp;#8217;s into the platform team&amp;#8217;s gitops repo. Once configured, the Tenant team is in control of their namespaces and can get on with managing their own products and tooling.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_secrets_management_with_argocd_vault_plugin&quot;&gt;Secrets Management with ArgoCD Vault Plugin&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To make this work at scale and in production within an organisation, the &quot;batteries&quot; for secrets management must be included! They are table stakes really. It&amp;#8217;s fiddly, but worth the effort.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;There are many ways to do secrets management beyond k8s secrets - &lt;a href=&quot;https://cloud.redhat.com/blog/a-guide-to-gitops-and-secret-management-with-argocd-operator-and-sops&quot;&gt;KSOPS&lt;/a&gt;, &lt;a href=&quot;https://external-secrets.io&quot;&gt;External Secrets Operator&lt;/a&gt; etc. The method i want to talk about uses the &lt;a href=&quot;https://argocd-vault-plugin.readthedocs.io/en/stable/backends/&quot;&gt;ArgoCD Vault Plugin&lt;/a&gt; which i will abbreviate to &lt;strong&gt;AVP&lt;/strong&gt;. It supports multiple secret backends by the way. In this case, i am going to use &lt;a href=&quot;https://developer.hashicorp.com/vault/docs/auth/kubernetes&quot;&gt;Hashicorp Vault&lt;/a&gt; and the k8s integration auth method. Setting up vault is dealt with &lt;a href=&quot;https://eformat.github.io/vault-quickstart/&quot;&gt;separately&lt;/a&gt; but can be done on-cluster or off-cluster.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;To get AVP working, you basically deploy the ArgoCD repo server with a ServiceAccount and use that secret to authenticate to Hashi Vault using k8s auth method. This way each ArgoCD instance uses the token associated with that service account to authenticate. Note that in OpenShift 4.11+ when creating new service accounts (SA), a service account token secret is &lt;a href=&quot;https://docs.openshift.com/container-platform/4.11/nodes/pods/nodes-pods-secrets.html#nodes-pods-secrets-creating-sa_nodes-pods-secrets&quot;&gt;no longer automatically generated.&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once done, our app secrets can be easily referenced from within source code using either annotations:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;kind: Secret
apiVersion: v1
metadata:
  name: example-secret
  annotations:
    avp.kubernetes.io/path: &quot;path/to/app-secret&quot;
type: Opaque
data:
  password: &amp;lt;password-vault-key&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;or directly via the full path:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;  password: &amp;lt;path:kv/data/path/to/app-secret#password-vault-key&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We can also reference the secrets directly from our ArgoCD Application definitions. Here is an example of using helm (kustomize and plain yaml are also supported).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;  source:
    repoURL: https://github.com/eformat/my-gitrepo.git
    path: gitops/my-app/chart
    targetRevision: main
    plugin:
      name: argocd-vault-plugin-helm
      env:
        - name: HELM_VALUES
          value: |
            image:
              repository: image-registry.openshift-image-registry.svc:5000/my-namespace/my-app
              tag: &quot;1.2.3&quot;
            password: &amp;lt;path:kv/data/path/to/app-secret#password-vault-key&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I also use a pattern to pass the &lt;em&gt;vault annotation path&lt;/em&gt; down to the helm chart from the ArgoCD Application. To keep things clean (and you sane!) I normally have a Vault secret per-application (containing many KV2 - key:value pairs).&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;    plugin:
      name: argocd-vault-plugin-helm
      env:
        - name: HELM_VALUES
          value: |
            resources:
              limits:
                cpu: 500m         # a non secret value
            avp:
              secretPath: &quot;kv/data/path/to/app-secret&quot;  # use this in the annotations&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This allows you to control the &lt;strong&gt;path&lt;/strong&gt; to your secrets in Vault which can be configured by convention e.g.  &lt;strong&gt;kv/data/cluster/namespace/app&lt;/strong&gt; as an example.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_argocd_configuration_the_gory_details&quot;&gt;ArgoCD Configuration - The Gory Details&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;OK, great. But how do i get there with my Team ArgoCD ? Let&amp;#8217;s take a look in depth at the &lt;a href=&quot;https://github.com/redhat-cop/helm-charts/blob/master/charts/gitops-operator/values.yaml&quot;&gt;&lt;strong&gt;argocd-values.yaml&lt;/strong&gt;&lt;/a&gt; file you might pass into the gitops-operator helm chart to bootstrap your ArgoCD.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The important bit for AVP integration is to mount the token from a service account that we have created - in this case the service account is called &lt;strong&gt;argocd-repo-vault&lt;/strong&gt; and we set &lt;strong&gt;mountastoken&lt;/strong&gt; to &quot;true&quot;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Next, we use an &lt;strong&gt;initContainer&lt;/strong&gt; to download the AVP go binary and save it to a &lt;strong&gt;custom-tools&lt;/strong&gt; directory. If you are doing this disconnected, the binary needs to be made available offline.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;argocd_cr:
  statusBadgeEnabled: true
  repo:
    mountsatoken: true
    serviceaccount: argocd-repo-vault
    volumes:
    - name: custom-tools
      emptyDir: {}
    initContainers:
    - name: download-tools
      image: registry.access.redhat.com/ubi8/ubi-minimal:latest
      command: [sh, -c]
      env:
        - name: AVP_VERSION
          value: &quot;1.11.0&quot;
      args:
        - &amp;gt;-
          curl -Lo /tmp/argocd-vault-plugin https://github.com/argoproj-labs/argocd-vault-plugin/releases/download/v\${AVP_VERSION}/argocd-vault-plugin_\${AVP_VERSION}_linux_amd64 &amp;amp;&amp;amp; chmod +x /tmp/argocd-vault-plugin &amp;amp;&amp;amp; mv /tmp/argocd-vault-plugin /custom-tools/
      volumeMounts:
      - mountPath: /custom-tools
        name: custom-tools
    volumeMounts:
    - mountPath: /usr/local/bin/argocd-vault-plugin
      name: custom-tools
      subPath: argocd-vault-plugin&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We need to create the &lt;em&gt;glue&lt;/em&gt; between our ArgoCD Applications&apos; and how they call/use the AVP binary. This is done using the &lt;strong&gt;configManagementPlugins&lt;/strong&gt; stanza. Note we use three methods, one for plain YAML, one for helm charts, one for kustomize. The plugin &lt;strong&gt;name:&lt;/strong&gt; is what we reference from our ArgoCD Application.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;  configManagementPlugins: |
    - name: argocd-vault-plugin
      generate:
        command: [&quot;sh&quot;, &quot;-c&quot;]
        args: [&quot;argocd-vault-plugin -s team-ci-cd:team-avp-credentials generate ./&quot;]
    - name: argocd-vault-plugin-helm
      init:
        command: [sh, -c]
        args: [&quot;helm dependency build&quot;]
      generate:
        command: [&quot;bash&quot;, &quot;-c&quot;]
        args: [&apos;helm template &quot;$ARGOCD_APP_NAME&quot; -n &quot;$ARGOCD_APP_NAMESPACE&quot; -f &amp;lt;(echo &quot;$ARGOCD_ENV_HELM_VALUES&quot;) . | argocd-vault-plugin generate -s team-ci-cd:team-avp-credentials -&apos;]
    - name: argocd-vault-plugin-kustomize
      generate:
        command: [&quot;sh&quot;, &quot;-c&quot;]
        args: [&quot;kustomize build . | argocd-vault-plugin -s team-ci-cd:team-avp-credentials generate -&quot;]&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We make use of environment variables set within the AVP plugin for helm so that the namespace and helm values from the ArgoCD Application are set correctly. See the &lt;a href=&quot;https://argocd-vault-plugin.readthedocs.io/en/stable/usage/&quot;&gt;AVP documentation&lt;/a&gt; for full details of usage.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;One thing to note, is the &lt;strong&gt;team-ci-cd:team-avp-credentials&lt;/strong&gt; secret. This specifies &lt;em&gt;how&lt;/em&gt; the AVP binary connects and authenticates to Hashi Vault. It is a secret that you need to set up. An example as follows for a simple hashi vault in-cluster deployment:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;export AVP_TYPE=vault
export VAULT_ADDR=https://vault-active.hashicorp.svc:8200   # vault url
export AVP_AUTH_TYPE=k8s                              # kubernetes auth
export AVP_K8S_ROLE=argocd-repo-vault                 # vault role (service account name)
export VAULT_SKIP_VERIFY=true
export AVP_MOUNT_PATH=auth/$BASE_DOMAIN-$PROJECT_NAME

cat &amp;lt;&amp;lt;EOF | oc apply -n ${PROJECT_NAME} -f-
---
apiVersion: v1
stringData:
  VAULT_ADDR: &quot;${VAULT_ADDR}&quot;
  VAULT_SKIP_VERIFY: &quot;${VAULT_SKIP_VERIFY}&quot;
  AVP_AUTH_TYPE: &quot;${AVP_AUTH_TYPE}&quot;
  AVP_K8S_ROLE: &quot;${AVP_K8S_ROLE}&quot;
  AVP_TYPE: &quot;${AVP_TYPE}&quot;
  AVP_K8S_MOUNT_PATH: &quot;${AVP_MOUNT_PATH}&quot;
kind: Secret
metadata:
  name: team-avp-credentials
  namespace: ${PROJECT_NAME}
type: Opaque
EOF&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;I am leaving out the gory details of Vault/ACL setup which are documented &lt;a href=&quot;https://eformat.github.io/vault-quickstart/&quot;&gt;elsewhere&lt;/a&gt;, however to create the auth secret in vault from the &lt;strong&gt;argocd-repo-vault&lt;/strong&gt; ServiceAccount token, i use this shell script:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;bash&quot;&gt;export SA_TOKEN=$(oc -n ${PROJECT_NAME} get sa/${APP_NAME} -o yaml | grep ${APP_NAME}-token | awk &apos;{print $3}&apos;)
export SA_JWT_TOKEN=$(oc -n ${PROJECT_NAME} get secret $SA_TOKEN -o jsonpath=&quot;{.data.token}&quot; | base64 --decode; echo)
export SA_CA_CRT=$(oc -n ${PROJECT_NAME} get secret $SA_TOKEN -o jsonpath=&quot;{.data[&apos;ca\.crt&apos;]}&quot; | base64 --decode; echo)

vault write auth/$BASE_DOMAIN-${PROJECT_NAME}/config \
  token_reviewer_jwt=&quot;$SA_JWT_TOKEN&quot; \
  kubernetes_host=&quot;$(oc whoami --show-server)&quot; \
  kubernetes_ca_cert=&quot;$SA_CA_CRT&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_why_do_all_of_this&quot;&gt;Why Do All of This ?&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;The benefit of all this gory configuration stuff:&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;ulist&quot;&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;we can now store secrets safely in a backend vault at enterprise scale&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we have all of our ArgoCD&amp;#8217;s use these secrets consistently with gitops in a multi-tenanted manner&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we keep secrets values out of our source code&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;we can control all of this with gitops&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It also means that the platform an product teams, can manage secrets in a safely consistent manner - but separately i.e. each team manages their own secrets and space in vault. This method also works if you are using the enterprise Hashi vault that uses &lt;strong&gt;namespaces&lt;/strong&gt; - you can just set the env.var into your ArgoCD Application like so.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;listingblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;pre class=&quot;prettyprint highlight&quot;&gt;&lt;code data-lang=&quot;yaml&quot;&gt;    plugin:
      name: argocd-vault-plugin-kustomize
      env:
        - name: VAULT_NAMESPACE
          value: &quot;my-team-apps&quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Tenant team&amp;#8217;s are now fully in control of their namespaces and secrets and can get on with managing their own applications, products and tools !&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>DevOps with OpenShift Introduction</title>
          <link>https://blog.eformat.me/2022/11/devops-with-openshift-5yr.html</link>
          <pubDate>Thu, 3 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/devops-with-openshift-5yr.html</guid>
          <description>
              &lt;div class=&quot;sect1 pagenumrestart&quot;&gt;
&lt;h2 id=&quot;Introduction-to-DevOps&quot;&gt;5 Years!&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;It&amp;#8217;s been five years since we wrote the inaugural DevOps with OpenShift book. I re-read the introduction recently, and thought &lt;em&gt;&quot;It hasn&amp;#8217;t aged that badly !&quot;&lt;/em&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;&lt;a href=&quot;https://cloud.redhat.com/hubfs/pdfs/DevOps_with_OpenShift.pdf&quot; class=&quot;bare&quot;&gt;https://cloud.redhat.com/hubfs/pdfs/DevOps_with_OpenShift.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;cover&quot; class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/2022/11/dowo_cover.png&quot; alt=&quot;DevOps with OpenShift&quot; width=&quot;640&quot; height=&quot;480&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;title&quot;&gt;Figure 1. DevOps with OpenShift&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect1&quot;&gt;
&lt;h2 id=&quot;_introduction_to_devops_with_openshift&quot;&gt;Introduction to DevOps with OpenShift&lt;/h2&gt;
&lt;div class=&quot;sectionbody&quot;&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;This book provides a practical guide for using OpenShift as an enablement technology for DevOps. OpenShift&amp;#8217;s combination of container management platform with natively container-aware automation can bring those Developer and Operations constituencies together in ways not previously possible. This enables software work products to present themselves in a standardized form to your preferred continuous integration and delivery tool chains.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Container awareness makes it possible to leverage deployment strategies and quality of service characteristics honored by the container management platform and underlying orchestration engine. We can start thinking in terms of &lt;em&gt;containers-as-code&lt;/em&gt; rather than &lt;em&gt;infrastructure-as-code&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;So to get started, let&amp;#8217;s review some key DevOps concepts as interpreted with a container-centric viewpoint.&lt;/p&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_devops&quot;&gt;DevOps&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;DevOps is concerned with aligning the constituents in the software delivery process to a common goal of value delivery&amp;#x2014;and it&amp;#8217;s not just Developers and Operators, but InfoSec and Quality Assurance functions and more. Recognize that wealth is created when the work product is valued by actors external to the production system. Value delivery outcomes are measured by metrics tied to production delivery velocity, quality, and waste. DevOps emphasizes behavioral- or cultural-related changes such as those which encourage teaming, inclusion, feedback, and experimentation. Technological interventions such as automation are central as they can reinforce such target behaviors. DevOps does not necessarily imply functional roles in software delivery such as development, quality assurance, or operations are merged or seconded. More important is that a professional respect and shared sensibility is formed across the delivery team.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_containers&quot;&gt;Containers&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Containers are the runtime representation of a packaging format based on a lightweight, immutable image. Runtime dependencies are resolved within the image which facilitates portability. This makes possible the agreement on a standardized software work product. Management and runtime tooling that is container aware can then be applied consistently no matter what the underlying technology stack. Container-based workloads are suitable for multi-tenancy on a single compute instance and when implemented securely can realize significant operation efficiencies. An important corollary is that launching a new workload does not incur the cost of provisioning new compute infrastructure. This enables a true on-demand, self-service experience for users.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_container_orchestration&quot;&gt;Container Orchestration&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Container orchestration involves the lifecycle management of container workloads, including functions such as to schedule, stop, start, and replicate across a cluster of machines. Compute resources for running workloads are abstracted, allowing the host infrastructure to be treated as a single logical deployment target. Kubernetes is an open source community project addressing container orchestration. It groups containers that make up an application into logical units for easy management and discovery, and features self-healing, service discovery, load balancing, and storage services among its rich feature set. Orchestration plays a critical role in our design goal of application-centricity as quality of service attributes and deployment patterns are executed by invoking Kubernetes API primitives.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_continuous_integration&quot;&gt;Continuous Integration&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Continuous integration (CI) concerns the integration of code from potentially multiple authors into a shared source code management (SCM) repository. Such check-ins could occur many times a day, and automation steps in such a process could include gates or controls to expose any issues as early as possible. SCMs such as Git include workflow support to commit to trunk, push, and merge code pull requests from multiple developers. With containers, a Git push event could be configured to then trigger an image build event via the webhooks mechanism.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_continuous_delivery&quot;&gt;Continuous Delivery&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Once a CI strategy is in place, consideration can then move to achieving continuous delivery (CD). This involves automating the steps required to promote the work product from one environment to the next within the defined software development lifecycle (SDLC). Such steps could include automated testing, smoke, unit, functional, and static code analysis and static dependency checks for known security vulnerabilities. With containers, promotion in later stages of the SLC may merely involve the tagging of the (immutable) image to mark acceptance. Binary promotions are also possible such that only the image is pushed (to the target registry of the new environment), leaving source code in situ.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_continuous_deployment&quot;&gt;Continuous Deployment&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;By convention, we can denote the special case of automated continuous delivery to production as &lt;em&gt;continuous deployment&lt;/em&gt; (CD). We make such a distinction because such deployments may be subject to additional governance processes and gates&amp;#x2014;for example, deliberate human intervention to manage risk and complete sign-off procedures. We make such a distinction because such deployments may be subject to additional governance processes. As per &lt;a href=&quot;#one_1&quot;&gt;Continuous delivery versus deployment&lt;/a&gt;, there may be scenarios for deliberate human intervention to manage risk and complete sign-off procedures.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;one_1&quot; class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/2022/11/dowo_0101.png&quot; alt=&quot;Continuous Delivery versus Deployment&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;title&quot;&gt;Figure 2. Continuous delivery versus deployment&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_pipelines&quot;&gt;Pipelines&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Pipelines are a representation of the flow/automation in a CI/CD process. Typically a pipeline might call out discrete steps in the software delivery process and present them visually or via a high-level scripting language so the flow can be manipulated. The steps might include build, unit tests, acceptance tests, packaging, documentation, reporting, and deployment and verification phases. Well-designed pipelines help deliver better quality code faster by enabling participants in the software delivery process to more easily diagnose and respond to feedback. As illustrated in &lt;a href=&quot;#one_2&quot;&gt;Smaller releases, release often, faster feedback&lt;/a&gt;, diagnosis and response turnaround can be accelerated by organizing releases into smaller and more frequent release bundles.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;one_2&quot; class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/2022/11/dowo_0102.png&quot; alt=&quot;Smaller, more frequent releases&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;title&quot;&gt;Figure 3. Smaller releases, release often, faster feedback&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_software_configuration_management&quot;&gt;Software Configuration Management&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;For our purposes we will take a narrower view of software configuration management (CM) and focus on the recommended software engineering practice of separating dynamic configuration from static runtime software. Doing so allows developers and operations engineers to change the configuration without having to rebuild the runtime such as might occur when deploying to different environments. Containers, based as they are on immutable images, amplify this behavior as the alternative would be configuration layered across multiple images for each deployment scenario.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_deployment_patterns&quot;&gt;Deployment Patterns&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Aligned with the goal of automation across all steps in the software delivery lifecycle are patterns for deployment. We look here for strategies that can balance across criteria including safety, testability, reversibility, and downtime minimization in cloud-scale scenarios. Some deployment patterns also offer opportunities for capturing and responding to feedback. An A/B deployment allows for testing a user-defined hypothesis such as whether application version A is more effective than B. Usage results can then drive weighted load balancing across the alternatives. Automation of deployment strategies in this DevOps world are implemented by driving the orchestration APIs.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_continuous_improvement&quot;&gt;Continuous Improvement&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;Let&amp;#8217;s conclude this chapter by covering continuous improvement (&lt;a href=&quot;#one_3&quot;&gt;Continuous improvement&lt;/a&gt;), which should be the thread that connects all of the process improvement&amp;#x2013;related practices summarized. The environment changes and so must we. These practices make it easy and inexpensive to experiment, formulate, and test hypotheses, as well as capture, act on, and experiment with the feedback received. This way we can continue to inject energy into the system and so maintain a state of dynamic stability&amp;#x2014;a balance of adaptive/agile versus fixed/stable.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&quot;one_3&quot; class=&quot;imageblock&quot;&gt;
&lt;div class=&quot;content&quot;&gt;
&lt;img src=&quot;/2022/11/dowo_0103.png&quot; alt=&quot;Continuous Improvement&quot;&gt;
&lt;/div&gt;
&lt;div class=&quot;title&quot;&gt;Figure 4. Continuous improvement&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&quot;sect2&quot;&gt;
&lt;h3 id=&quot;_summary&quot;&gt;Summary&lt;/h3&gt;
&lt;div class=&quot;paragraph&quot;&gt;
&lt;p&gt;We covered here some of what is unique and nuanced about DevOps with OpenShift and why it matters. Realizing these DevOps concepts using natively container-aware automation can bring cloud deployment power to &lt;em&gt;all&lt;/em&gt; the people, from 10x programmer to citizen developer. The following chapters will show you how.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
          </description>
      </item>
      
      <item>
          <title>Pulsar Flink</title>
          <link>https://blog.eformat.me/2022/11/pulsar-flink.html</link>
          <pubDate>Wed, 2 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/pulsar-flink.html</guid>
          <description>
              &lt;h1&gt;&lt;a href=&quot;#pulsar-flink&quot; id=&quot;pulsar-flink&quot;&gt;Pulsar Flink&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;I have been messing around with yet another streaming demo (YASD). You really just cannot have too many. 🤩&lt;/p&gt;
&lt;p&gt;I am a fan of &lt;a href=&quot;https://en.wikipedia.org/wiki/Server-sent_events&quot;&gt;server sent events&lt;/a&gt;, why ? because they are HTML5 native. No messing around with web sockets. I have a a &lt;a href=&quot;https://github.com/eformat/quote-generator&quot;&gt;small quarkus app&lt;/a&gt; that generates stock quotes:&lt;/p&gt;
&lt;div id=&quot;lightbox&quot;&gt;&lt;/div&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/eformat/quote-generator/master/images/quotes.gif&quot; width=&quot;500&quot; class=&quot;zoom&quot;&gt;
&lt;p&gt;that you can easily run locally or on OpenShift:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;oc new-app quay.io/eformat/quote-generator:latest
oc create route edge quote-generator --service=quote-generator --port=8080
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;and then retrieve the events in the browser or by curl:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;curl -H &amp;quot;Content-Type: application/json&amp;quot; --max-time 9999999 -N http://localhost:8080/quotes/stream
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;So, first challenge - How might we consume these SSE&amp;rsquo;s using Flink? I found a handy &lt;a href=&quot;https://github.com/aws-samples/amazon-kinesis-data-analytics-apache-flink-server-sent-events-sse/tree/main/kinesis-data-analytics-sse&quot;&gt;AWS Kinesis SSE demo&lt;/a&gt; which i snarfed the SSE/OKHttp code from. I wired this into flinks &lt;code&gt;RichSourceFunction&lt;/code&gt;:&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/4d5fd40d8566e99e866e1e7fd4cb6388.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;So now i could consume this SSE source as a &lt;code&gt;DataStream&lt;/code&gt;&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/c63c765710b00b9ce15201edd9aca87b.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;In the example, i wire in the stock quotes for &lt;code&gt;NFLX&lt;/code&gt; and &lt;code&gt;RHT&lt;/code&gt;. Next step, process these streams. Since i am new to flink, i started with a simple print function, then read this &lt;a href=&quot;https://flink.apache.org/news/2015/02/09/streaming-example.html&quot;&gt;stock price&lt;/a&gt; example from 2015! cool. So i implemented a simple &lt;code&gt;BuyFunction&lt;/code&gt; class that makes stock buy recommendations:&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/156cabbd95543e22f4faf90f9529a192.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;Lastly, it needs to be sent to a sink. Again, i started by using a simple print sink:&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/783e1d4a37bc33e91393416109a92b67.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;Friends of mine have been telling me how much more awesome &lt;code&gt;Pulsar&lt;/code&gt; is compared to &lt;code&gt;Kafka&lt;/code&gt; so i also tried out sending to a local pulsar container that you can run using:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;podman run -it -p 6650:6650 -p 8081:8080 --rm --name pulsar docker.io/apachepulsar/pulsar:2.10.2 bin/pulsar standalone
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And forwrded to pulsar using a simple utility class using the pulsar java client:&lt;/p&gt;
&lt;script src=&quot;https://gist.github.com/eformat/90121414185b9142d884b72cb1e7af1c.js&quot;&gt;&lt;/script&gt;
&lt;p&gt;Then consume the messages to make sure they are there !&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-bash&quot;&gt;podman exec -i pulsar bin/pulsar-client consume -s my-subscription -n 0 persistent://public/default/orders
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;And i need to write this post as well .. getting it to run in OpenShift &amp;hellip;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/2022/11/flink-stocks-ocp.png&quot; width=&quot;640&quot; class=&quot;zoom&quot;&gt;&lt;/p&gt;
&lt;p&gt;&lt;/br&gt;&lt;br /&gt;
Source code is here - &lt;a href=&quot;https://github.com/eformat/flink-stocks&quot;&gt;https://github.com/eformat/flink-stocks&lt;/a&gt;&lt;/p&gt;

          </description>
      </item>
      
      <item>
          <title>The Compelling Platform</title>
          <link>https://blog.eformat.me/2022/11/the-compelling-platform.html</link>
          <pubDate>Tue, 1 Nov 2022 00:00:00 +0000</pubDate>
          <guid isPermaLink="false">https://blog.eformat.me/2022/11/the-compelling-platform.html</guid>
          <description>
              &lt;h1&gt;&lt;a href=&quot;#the-compelling-platform&quot; id=&quot;the-compelling-platform&quot;&gt;The Compelling Platform&lt;/a&gt;&lt;/h1&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;Build it and they will come !&amp;rdquo;&lt;/em&gt; - 1989 movie Field of Dreams.&lt;/p&gt;
&lt;p&gt;A &lt;em&gt;key&lt;/em&gt; ingredient for success when building a platform is that it must be &lt;em&gt;compelling&lt;/em&gt; to use. What makes a platform compelling ?&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The platform is self-service for the overwhelming majority of use cases.&lt;/li&gt;
&lt;li&gt;The platform is composable, containing discrete services that can be used independently.&lt;/li&gt;
&lt;li&gt;The platform does not force an inflexible way of working upon the delivery team.&lt;/li&gt;
&lt;li&gt;The platform is quick and cheap to start using, with an easy on-ramp (e.g. Quick start guides, documentation, code samples)&lt;/li&gt;
&lt;li&gt;The platform has a rich internal user community for sharing&lt;/li&gt;
&lt;li&gt;The platform is secure and compliant by default&lt;/li&gt;
&lt;li&gt;The platform is up to date&lt;/li&gt;
&lt;li&gt;The platform is the thinnest viable&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A platform should also be more than just software and APIs - it is documentation, and consulting, and support and evangelism, and templates and guidelines.&lt;/p&gt;
&lt;p&gt;You must also move away from &lt;em&gt;project&lt;/em&gt; as the primary mechanism for funding and staffing delivery of technology. Platform is a &lt;em&gt;product&lt;/em&gt;, and needs a long-lived and stable team tasked with both build and run.&lt;/p&gt;
&lt;h2&gt;&lt;a href=&quot;#define-platform&quot; id=&quot;define-platform&quot;&gt;Define Platform&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;A platform is a curated experience for the customer of the platform (engineers)&amp;rdquo;&lt;/em&gt; - Matthew Skelton.&lt;/p&gt;
&lt;p&gt;&lt;em&gt;&amp;ldquo;A digital platform is a foundation of self-service APIs, tools, services, knowledge and support which are arranged as a compelling internal product. Autonomous delivery teams can make use of the platform to deliver product features at a higher pace, with reduced co-ordination.&amp;rdquo;&lt;/em&gt; - Evan Botcher.&lt;/p&gt;
&lt;p&gt;What it is &lt;b&gt;NOT:&lt;/b&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;It is not the limited virtualised hosting and locked-down centrally-managed tooling that you already have.&lt;/li&gt;
&lt;li&gt;It is not just OpenShift, Ansible or RHEL by themselves.&lt;/li&gt;
&lt;li&gt;The &lt;em&gt;fattest&lt;/em&gt; platform in the world.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The starting point is to &lt;em&gt;&amp;ldquo;Use these N services in these ways &amp;hellip;&amp;rdquo;&lt;/em&gt; - this is a curated experience.&lt;/p&gt;
&lt;p&gt;The &lt;em&gt;Thinnest Viable Platform&lt;/em&gt; is a small, curated set of complementary services or patterns used together to simplify and accelerate delivery.&lt;/p&gt;
&lt;p&gt;The platform will evolve and its design should meet common team interaction modes.&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/eformat/the-compelling-platform/blob/main/PATTERNS.md&quot;&gt;Patterns for the Compelling Platform &amp;gt;&lt;/a&gt;&lt;/p&gt;

          </description>
      </item>
      

  </channel> 
</rss>
